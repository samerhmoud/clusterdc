











import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, Union, List, Dict, Tuple
from pathlib import Path


class Data:
    """
    A comprehensive data handling and visualization utility class that provides functionality 
    for loading, analyzing, and visualizing data from various file formats, with additional 
    capabilities for density estimation and cluster analysis.
    """
    def __init__(self):
        """
        Initializes a new Data object with empty placeholders for data, file path, file type,
        KDE model, and sample weights. Establishes a fixed random state (42) for reproducibility.
        """
        self.data = None
        self.file_path = None
        self.file_type = None
        self.kde_model = None
        self.sample_weights = None
        
        # Set fixed random state for reproducibility
        np.random.seed(42)
        self.random_state = 42
        
    def read_file(self, 
                  file_path: str, 
                  sheet_name: Optional[str] = None,
                  **kwargs) -> pd.DataFrame:
        """
        Reads data from CSV or Excel files into a pandas DataFrame.

        Parameters:
            file_path (str): Path to the file to be read
            sheet_name (Optional[str]): Name of the sheet for Excel files
            **kwargs: Additional parameters to pass to pandas read functions

        Returns:
            pd.DataFrame: The loaded DataFrame

        Raises:
            ValueError: If file format is not supported
            Exception: If there are errors during file reading
        """
        self.file_path = Path(file_path)
        self.file_type = self.file_path.suffix.lower()
        
        try:
            if self.file_type == '.csv':
                self.data = pd.read_csv(file_path, **kwargs)
            elif self.file_type in ['.xlsx', '.xls']:
                self.data = pd.read_excel(file_path, sheet_name=sheet_name, **kwargs)
            else:
                raise ValueError(f"Unsupported file format: {self.file_type}")
                
            print(f"Successfully loaded data from {self.file_path}")
            print(f"Shape: {self.data.shape}")
            return self.data
            
        except Exception as e:
            print(f"Error reading file: {str(e)}")
            raise
    
    def get_summary(self) -> pd.DataFrame:
        """
        Generates a comprehensive summary of the loaded data.

        Returns:
            pd.DataFrame: A DataFrame containing:
                - Data types for each column
                - Count of values
                - Missing values (count and percentage)
                - Unique value counts
                - For numeric columns: mean, std, min, quartiles, max (rounded to 3 decimals)

        Raises:
            ValueError: If no data has been loaded
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")
            
        # Initialize summary dictionary
        summary_dict = {
            'dtype': self.data.dtypes,
            'count': self.data.count(),
            'missing': self.data.isna().sum(),
            'missing_%': (self.data.isna().sum() / len(self.data) * 100).round(2),
            'unique': self.data.nunique()
        }
        
        # Create initial DataFrame
        summary_df = pd.DataFrame(summary_dict)
        
        # Get numeric columns
        numeric_cols = self.data.select_dtypes(include=['int64', 'float64']).columns
        
        # Add numeric statistics where applicable
        if len(numeric_cols) > 0:
            desc = self.data[numeric_cols].describe()
            summary_df.loc[numeric_cols, 'mean'] = desc.loc['mean']
            summary_df.loc[numeric_cols, 'std'] = desc.loc['std']
            summary_df.loc[numeric_cols, 'min'] = desc.loc['min']
            summary_df.loc[numeric_cols, '25%'] = desc.loc['25%']
            summary_df.loc[numeric_cols, '50%'] = desc.loc['50%']
            summary_df.loc[numeric_cols, '75%'] = desc.loc['75%']
            summary_df.loc[numeric_cols, 'max'] = desc.loc['max']
            
        # Round numeric columns
        numeric_summary_cols = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']
        summary_df[numeric_summary_cols] = summary_df[numeric_summary_cols].round(3)
        
        return summary_df

    def save_data(self, 
                  file_path: str,
                  sheet_name: Optional[str] = None,
                  index: bool = False,
                  **kwargs) -> None:
        """
        Saves the current data to a file in CSV or Excel format.

        Parameters:
            file_path (str): Path where the file should be saved
            sheet_name (Optional[str]): Name of the sheet for Excel files
            index (bool): Whether to include index in the output file
            **kwargs: Additional parameters to pass to pandas write functions

        Raises:
            ValueError: If no data has been loaded or file format is not supported
            Exception: If there are errors during file saving
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")
            
        save_path = Path(file_path)
        file_type = save_path.suffix.lower()
        
        try:
            if file_type == '.csv':
                self.data.to_csv(file_path, index=index, **kwargs)
            elif file_type in ['.xlsx', '.xls']:
                self.data.to_excel(file_path, 
                                 sheet_name=sheet_name or 'Sheet1',
                                 index=index, 
                                 **kwargs)
            else:
                raise ValueError(f"Unsupported file format: {file_type}")
                
            print(f"Successfully saved data to {save_path}")
            
        except Exception as e:
            print(f"Error saving file: {str(e)}")
            raise
    
    def plot_scatter(self,
                    x: str,
                    y: str,
                    labels: Optional[Union[str, List[int], List[str]]] = None,
                    title: Optional[str] = None,
                    figsize: Tuple[int, int] = (10, 8),
                    save_path: Optional[str] = None,
                    buffer_fraction: float = 0.05,
                    point_size: int = 10,
                    alpha: float = 0.6,
                    dpi: int = 300,
                    show_kde: bool = False,
                    kernel_type: str = 'gaussian'):
        """
        Creates a customizable scatter plot of two variables with optional features.

        Parameters:
            x (str): Column name for x-axis
            y (str): Column name for y-axis
            labels (Optional[Union[str, List[int], List[str]]]): Labels for color-coding points
            title (Optional[str]): Plot title
            figsize (Tuple[int, int]): Figure size in inches
            save_path (Optional[str]): Path to save the plot
            buffer_fraction (float): Fraction of range to add as buffer around plot
            point_size (int): Size of scatter points
            alpha (float): Transparency of points
            dpi (int): DPI for saved figure
            show_kde (bool): Whether to show kernel density estimation
            kernel_type (str): Type of kernel for density estimation

        Raises:
            ValueError: If no data has been loaded or specified columns not found
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")

        if x not in self.data.columns or y not in self.data.columns:
            raise ValueError(f"Columns {x} or {y} not found in data")

        # Create figure and axis objects explicitly
        fig, ax = plt.subplots(figsize=figsize)

        # Calculate axis limits with buffer
        x_min, x_max = self.data[x].min(), self.data[x].max()
        y_min, y_max = self.data[y].min(), self.data[y].max()

        x_range = x_max - x_min
        y_range = y_max - y_min

        x_buffer = buffer_fraction * x_range
        y_buffer = buffer_fraction * y_range

        plt.xlim(x_min - x_buffer, x_max + x_buffer)
        plt.ylim(y_min - y_buffer, y_max + y_buffer)

        if show_kde:
            # Initialize and fit KDE with Scott's bandwidth
            kde = KDE(data=self.data[[x, y]].values, kernel_types=[kernel_type])
            kde.fit(method='scott')

            # Get point densities
            densities = kde.get_point_densities()

            # Create scatter plot colored by density
            scatter = plt.scatter(self.data[x], 
                                self.data[y], 
                                c=densities,
                                cmap='viridis',
                                alpha=alpha,
                                s=point_size)
            plt.colorbar(scatter, label='Density (Scott\'s Rule)')

        elif labels is not None:
            # Handle different types of labels
            if isinstance(labels, str):
                if labels not in self.data.columns:
                    raise ValueError(f"Labels column {labels} not found in data")
                label_values = self.data[labels]
            else:
                if len(labels) != len(self.data):
                    raise ValueError("Length of labels must match length of data")
                label_values = labels

            # Get unique labels and sort them for consistent colors
            unique_labels = sorted(np.unique(label_values))
            n_clusters = len(unique_labels)
            colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))

            # Create legend elements
            legend_elements = [
                plt.Line2D([0], [0], 
                          marker='o', 
                          color='w',
                          markerfacecolor=colors[i], 
                          label=f'Cluster {cluster}',
                          markersize=10)
                for i, cluster in enumerate(unique_labels)
            ]

            # Plot points for each cluster
            for label, color in zip(unique_labels, colors):
                mask = label_values == label
                plt.scatter(self.data[x][mask], 
                           self.data[y][mask],
                           c=[color], 
                           alpha=alpha,
                           s=point_size)

            # Calculate number of legend columns needed (15 items per column)
            n_cols = (n_clusters - 1) // 15 + 1

            # Add legend with dynamic columns
            if n_cols > 1:
                plt.legend(handles=legend_elements,
                          loc='center left',
                          bbox_to_anchor=(1, 0.5),
                          ncol=n_cols,
                          columnspacing=1.5,
                          handletextpad=0.5)
            else:
                plt.legend(handles=legend_elements,
                          loc='center left',
                          bbox_to_anchor=(1, 0.5))

        else:
            plt.scatter(self.data[x], 
                       self.data[y], 
                       alpha=alpha,
                       s=point_size,
                       c='black')

        # Set labels and title
        plt.xlabel(x)
        plt.ylabel(y)
        plot_title = title or f"Scatter Plot of {y} vs {x}"
        if show_kde:
            plot_title += "\nwith Density Estimate (Scott's Rule)"
        plt.title(plot_title)

        # Add grid
        plt.grid(True, linestyle='--', alpha=0.3)

        # Adjust layout to prevent label cutoff
        plt.tight_layout()

        # Save if path provided
        if save_path:
            plt.savefig(save_path, dpi=dpi, bbox_inches='tight')
            

    def estimate_density(self, 
                        columns: Optional[List[str]] = None,
                        kernel_type: str = 'gaussian') -> np.ndarray:
        """
        Estimates the density of points in the dataset using Kernel Density Estimation.

        Parameters:
            columns (Optional[List[str]]): Columns to use for density estimation
            kernel_type (str): Type of kernel to use for density estimation

        Returns:
            np.ndarray: Array of density estimates for each point

        Raises:
            ValueError: If no data has been loaded or columns are invalid
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")

        # Select numeric columns if none specified
        if columns is None:
            numeric_cols = self.data.select_dtypes(include=['float64', 'int64']).columns
            columns = numeric_cols.tolist()

        # Validate columns
        for col in columns:
            if col not in self.data.columns:
                raise ValueError(f"Column {col} not found in data")
            if not pd.api.types.is_numeric_dtype(self.data[col]):
                raise ValueError(f"Column {col} is not numeric")

        # Get data for selected columns
        X = self.data[columns].values

        # Set random state before initializing KDE
        np.random.seed(self.random_state)

        # Initialize and fit KDE with Scott's bandwidth
        self.kde_model = KDE(data=X, kernel_types=[kernel_type])
        self.kde_model.fit(method='scott')

        # Get point densities
        densities = self.kde_model.get_point_densities()

        # Normalize to get proper weights
        self.sample_weights = densities / densities.sum()

        return densities

    def sample(self, 
              n_samples: int,
              method: str = 'random',
              columns: Optional[List[str]] = None,
              kernel_type: str = 'gaussian',
              replace: bool = True,
              random_state: Optional[int] = None) -> pd.DataFrame:
        """
        Sample data points using either random or density-based sampling.

        Parameters:
            n_samples (int): Number of samples to draw
            method (str): Sampling method ('random' or 'density')
            columns (Optional[List[str]]): Columns to use for density estimation (only for density sampling)
            kernel_type (str): Type of kernel to use for density estimation (only for density sampling)
            replace (bool): Whether to sample with replacement (default True)
            random_state (Optional[int]): Random state for reproducibility. If None, uses class random_state

        Returns:
            pd.DataFrame: Sampled data

        Raises:
            ValueError: If no data has been loaded or invalid method specified

        Notes:
            - If random_state is not provided, uses the class's random_state (default=42)
            - Setting a specific random_state allows for reproducible sampling results
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")

        if method not in ['random', 'density']:
            raise ValueError("Method must be either 'random' or 'density'")

        # Set random state for reproducibility
        # If random_state is provided, use it; otherwise use class random_state
        current_random_state = random_state if random_state is not None else self.random_state
        np.random.seed(current_random_state)

        if method == 'density':
            if self.sample_weights is None:
                self.estimate_density(columns=columns, kernel_type=kernel_type)

            # Sample indices based on density weights
            sampled_indices = np.random.choice(
                len(self.data),
                size=n_samples,
                p=self.sample_weights,
                replace=replace
            )
        else:  # random sampling
            sampled_indices = np.random.choice(
                len(self.data),
                size=n_samples,
                replace=replace
            )

        # Return sampled data
        return self.data.iloc[sampled_indices].reset_index(drop=True)

    def plot_density_samples(self,
                            x: str,
                            y: str,
                            n_samples: int,
                            kernel_type: str = 'gaussian',
                            figsize: Tuple[int, int] = (15, 5),
                            random_state: Optional[int] = None,
                            return_samples: bool = False) -> Optional[pd.DataFrame]:
        """
        Creates a three-panel visualization comparing original and sampled data.

        Parameters:
            x (str): Column name for x-axis
            y (str): Column name for y-axis
            n_samples (int): Number of samples to draw
            kernel_type (str): Type of kernel for density estimation
            figsize (Tuple[int, int]): Figure size in inches
            random_state (Optional[int]): Random state for reproducibility. If None, uses class random_state
            return_samples (bool): If True, returns the sampled DataFrame

        Returns:
            Optional[pd.DataFrame]: If return_samples is True, returns the sampled DataFrame

        Raises:
            ValueError: If no data has been loaded
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")

        # Set random state for reproducibility
        current_random_state = random_state if random_state is not None else self.random_state
        np.random.seed(current_random_state)

        # Estimate density if not already done
        if self.sample_weights is None:
            self.estimate_density(columns=[x, y], kernel_type=kernel_type)

        # Get sampled data using density method
        sampled_data = self.sample(
            n_samples=n_samples,
            method='density',
            columns=[x, y],
            kernel_type=kernel_type,
            random_state=current_random_state
        )

        # Set random state before second KDE
        np.random.seed(current_random_state)

        # Estimate density for sampled data
        sampled_kde = KDE(data=sampled_data[[x, y]].values, kernel_types=[kernel_type])
        sampled_kde.fit(method='scott')
        sampled_weights = sampled_kde.get_point_densities()
        sampled_weights = sampled_weights / sampled_weights.sum()

        # Create plots
        fig, axes = plt.subplots(1, 3, figsize=figsize)

        # Original data (black points)
        axes[0].scatter(self.data[x], self.data[y], alpha=0.5, s=20, c='black')
        axes[0].set_title('Original Data')
        axes[0].set_xlabel(x)
        axes[0].set_ylabel(y)

        # Original data density plot
        scatter = axes[1].scatter(
            self.data[x], 
            self.data[y],
            c=self.sample_weights,
            cmap='viridis',
            alpha=0.5,
            s=20
        )
        plt.colorbar(scatter, ax=axes[1], label='Density')
        axes[1].set_title('Original Data \nKernel Density Estimate (Scott\'s Rule)')
        axes[1].set_xlabel(x)
        axes[1].set_ylabel(y)

        # Sampled data with density estimation
        scatter = axes[2].scatter(
            sampled_data[x], 
            sampled_data[y], 
            c=sampled_weights,
            cmap='viridis',
            alpha=0.5, 
            s=20
        )
        plt.colorbar(scatter, ax=axes[2], label='Density')
        axes[2].set_title(f'Sampled Data (n={n_samples}) \nKernel Density Estimate (Scott\'s Rule)')
        axes[2].set_xlabel(x)
        axes[2].set_ylabel(y)

        plt.tight_layout()

        if return_samples:
            return sampled_data
    
    def assign_nearest_cluster(self,
                             sampled_data: pd.DataFrame,
                             cluster_labels: Union[List[int], np.ndarray],
                             columns: List[str],
                             k_neighbors: int = 1) -> np.ndarray:
        """
        Assigns cluster labels to the original dataset by:
        1. First matching exact coordinates using pandas merge
        2. Then using k-nearest neighbors from original sampled points for remaining points,
           assigning labels based on the mode of the k neighbors' labels.

        Parameters:
            sampled_data: DataFrame containing the sampled points
            cluster_labels: Labels for the sampled points
            columns: List of column names used for matching
            k_neighbors: Number of nearest neighbors to consider (default=1)
                        Mode of their labels will be used for assignment

        Returns:
            np.ndarray: Array of cluster assignments
        """
        if self.data is None:
            raise ValueError("No data loaded. Please load data first.")

        if len(sampled_data) != len(cluster_labels):
            raise ValueError("Length of sampled_data and cluster_labels must match")

        if k_neighbors < 1:
            raise ValueError("k_neighbors must be at least 1")

        # Set random state for reproducibility
        np.random.seed(self.random_state)

        # Add cluster labels to sampled data and drop duplicates
        sampled_with_labels = sampled_data.copy()
        sampled_with_labels['cluster'] = cluster_labels.astype(int)
        sampled_with_labels = sampled_with_labels.drop_duplicates(subset=columns)

        # First attempt: exact coordinate matching
        result_df = self.data.copy()
        result_df['cluster'] = pd.NA

        # Merge to get exact matches
        for idx, row in self.data.iterrows():
            matches = sampled_with_labels[
                (sampled_with_labels[columns[0]] == row[columns[0]]) & 
                (sampled_with_labels[columns[1]] == row[columns[1]])
            ]
            if len(matches) > 0:
                result_df.loc[idx, 'cluster'] = matches.iloc[0]['cluster']

        # Find rows with no labels
        mask_no_label = result_df['cluster'].isna()
        rows_no_label = self.data[mask_no_label]

        if len(rows_no_label) > 0:
            print(f"Found {len(rows_no_label)} points without exact matches. Using {k_neighbors}-nearest neighbors for these points.")

            # Ensure k_neighbors doesn't exceed number of sampled points
            k = min(k_neighbors, len(sampled_with_labels))
            if k < k_neighbors:
                print(f"Warning: Reduced k_neighbors to {k} due to sample size limitation")

            # Build KD-tree from original sampled points
            tree = KDTree(sampled_with_labels[columns].values)

            # Find k nearest sampled points for each unlabeled point
            _, indices = tree.query(rows_no_label[columns].values, k=k)

            # For each unlabeled point, get the mode of its neighbors' labels
            for i, idx in enumerate(mask_no_label[mask_no_label].index):
                neighbor_labels = sampled_with_labels['cluster'].values[indices[i]]

                # Get unique labels and their counts
                unique_labels, counts = np.unique(neighbor_labels, return_counts=True)

                # Find labels with maximum count
                max_count = np.max(counts)
                max_labels = unique_labels[counts == max_count]

                # If multiple modes exist, randomly choose one (with fixed random state)
                if len(max_labels) > 1:
                    result_df.loc[idx, 'cluster'] = np.random.choice(max_labels)
                else:
                    result_df.loc[idx, 'cluster'] = max_labels[0]

        return result_df['cluster'].values.astype(int)








import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import NearestNeighbors
from skopt import gp_minimize
from skopt.space import Integer, Categorical, Real
from skopt.utils import use_named_args
from tqdm import tqdm
from joblib import Parallel, delayed
from scipy.linalg import inv, det
from collections import namedtuple
import pandas as pd

OptimizationResult = namedtuple(
    "OptimizationResult",
    ["method_used", "kernel_type", "k", "scaling_factor"]
)

class KDE:
    """
    Kernel Density Estimation (KDE) with Adaptive and Global Bandwidth Selection.
    
    This class implements Kernel Density Estimation with support for both adaptive 
    (local) and global bandwidth selection methods. It includes:
    - Multiple kernel functions (gaussian, epanechnikov, laplacian)
    - Automatic bandwidth selection using Bayesian optimization
    - Rule-of-thumb bandwidth methods (Scott's rule, Silverman's rule)
    - Visualization tools for density estimation and optimization progress
    
    The class supports both univariate and multivariate data, with specialized 
    optimizations for 2D visualizations. It uses Bayesian optimization to automatically 
    select optimal parameters for bandwidth and kernel type.
    
    Key Features:
    - Adaptive local bandwidth based on k-nearest neighbors
    - Global bandwidth with anisotropic covariance estimation
    - Multiple kernel functions for different types of data
    - Built-in visualization and optimization monitoring
    - Parallel processing for performance optimization
    """

    def __init__(self, data, k_limits=(1, 40), kernel_types=None, n_iter=50, save_path=None):
        """
        Initialize the KDE class.

        Parameters:
            data: Input dataset in one of these formats:
                - pd.DataFrame: DataFrame with numeric columns
                - np.ndarray: Array with shape (n_samples, n_features)
            k_limits (tuple): Range of neighbors (min%, max%) for local bandwidth as percentage of data points
            kernel_types (list): List of kernel functions. Options: ['gaussian', 'epanechnikov', 'laplacian']
            n_iter (int): Number of iterations for Bayesian optimization
            save_path (str, optional): Path to save visualizations

        Returns:
            None
        """
        # Handle DataFrame input
        if isinstance(data, pd.DataFrame):
            # Check for non-numeric columns
            non_numeric_cols = data.select_dtypes(exclude=['int64', 'float64']).columns
            if len(non_numeric_cols) > 0:
                raise ValueError(f"DataFrame contains non-numeric columns: {list(non_numeric_cols)}")
            
            # Convert DataFrame to numpy array
            self.data = data.to_numpy()
            self.column_names = data.columns.tolist()
        else:
            # Handle numpy array input
            if not isinstance(data, np.ndarray):
                raise ValueError("Input data must be either pandas DataFrame or numpy array")
            self.data = data
            self.column_names = [f"Feature_{i}" for i in range(data.shape[1])]
        self.k_limits = (int(len(data) * (k_limits[0] / 100)), int(len(data) * (k_limits[1] / 100)))
        self.kernel_types = kernel_types or ['gaussian', 'epanechnikov', 'laplacian']
        self.n_iter = n_iter
        self.best_kernel = None
        self.best_bandwidth = None
        self.best_scaling_factor = 1.0
        self.best_k = None
        self.method_used = None
        self.point_densities = None
        self.grid_densities = None
        self.iteration_results = None
        self.save_path = save_path
        self.optimization_progress = []
        
        # Add storage for grid calculation results
        self.stored_grid = None
        self.stored_grid_resolution = None
        self.stored_grid_buffer = None
        self.stored_grid_boundaries = None

    @staticmethod
    def _kernel_function(diff, inv_bandwidth, kernel_type):
        """
        Compute kernel density values using specified kernel type.

        Parameters:
            diff (np.ndarray): Difference matrix between points, shape (n_samples, n_features)
            inv_bandwidth (np.ndarray): Inverse of bandwidth matrix
            kernel_type (str): Type of kernel ('gaussian', 'epanechnikov', 'laplacian')

        Returns:
            np.ndarray: Kernel density values for each point
        """
        if diff.ndim == 1:
            diff = diff.reshape(1, -1)

        d = diff.shape[1]
        quadratic_form = np.einsum('ij,ji->i', np.dot(diff, inv_bandwidth), diff.T)

        if kernel_type == 'gaussian':
            coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
            return coefficient * np.exp(-0.5 * quadratic_form)

        elif kernel_type == 'epanechnikov':
            valid_mask = quadratic_form <= 1
            coefficient = (d + 2) / (2 * np.pi * det(inv_bandwidth) ** 0.5)
            return np.where(valid_mask, coefficient * (1 - quadratic_form), 0)

        elif kernel_type == 'laplacian':
            coefficient = 1 / (2 * np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
            return coefficient * np.exp(-np.sqrt(quadratic_form))

        else:
            raise ValueError(f"Unsupported kernel type: {kernel_type}")

    def _calculate_local_bandwidths(self, k):
        """
        Calculate local bandwidth matrices for each point using k-nearest neighbors.

        Parameters:
            k (int): Number of nearest neighbors to use

        Returns:
            list: List of local bandwidth matrices for each point
        """
        nbrs = NearestNeighbors(n_neighbors=k).fit(self.data)
        _, indices = nbrs.kneighbors(self.data)

        bandwidths = []
        for i in range(len(self.data)):
            local_points = self.data[indices[i]]
            local_cov = np.cov(local_points, rowvar=False) + np.eye(local_points.shape[1]) * 1e-8
            local_cov /= np.trace(local_cov)
            bandwidths.append(local_cov * self.best_scaling_factor)

        return bandwidths

    def _calculate_global_bandwidth(self, method=None):
        """
        Calculate global bandwidth matrix using specified method.

        Parameters:
            method (str, optional): Method for bandwidth calculation ('scott', 'silverman', or None)

        Returns:
            np.ndarray: Global bandwidth matrix
        """
        # Calculate covariance with stability check
        covariance_matrix = np.cov(self.data, rowvar=False)

        # Add small constant to diagonal for numerical stability
        covariance_matrix += np.eye(covariance_matrix.shape[0]) * 1e-10

        if method in ['scott', 'silverman']:
            n, d = self.data.shape

            # Standard deviation scaling
            std_dev = np.sqrt(np.diag(covariance_matrix))
            iqr = np.array([np.percentile(self.data[:, i], 75) - 
                            np.percentile(self.data[:, i], 25) 
                            for i in range(d)])

            # Use minimum of std and IQR/1.34 (robust estimate)
            sigma = np.minimum(std_dev, iqr/1.34)

            if method == 'scott':
                # Scott's rule with robust scaling
                scaling_factor = n ** (-1 / (d + 4))
            elif method == 'silverman':
                # Silverman's rule with robust scaling
                scaling_factor = (4 / (d + 2)) ** (1 / (d + 4)) * n ** (-1 / (d + 4))

            self.best_scaling_factor = scaling_factor

            # Scale covariance by robust estimates
            bandwidth_matrix = np.diag(sigma) @ \
                             (covariance_matrix / np.outer(std_dev, std_dev)) @ \
                             np.diag(sigma) * scaling_factor

        else:
            bandwidth_matrix = covariance_matrix * self.best_scaling_factor

        return bandwidth_matrix

    def _determine_optimal_batch_size(self, data_shape, safety_factor=0.05):
        """
        Determine optimal batch size based on available system memory.

        Parameters:
            data_shape: Shape of the full data array
            safety_factor: Fraction of available memory to leave as buffer (0.0-1.0)

        Returns:
            int: Optimal batch size
        """
        try:
            import psutil

            # Get available memory in bytes
            available_memory = psutil.virtual_memory().available

            # Apply safety factor to leave some memory free
            usable_memory = available_memory * (1 - safety_factor)

            # Calculate memory per row in the diff calculation
            # For each row, we need to store differences with all data points
            # Each difference is a 2D point (2 float64 values)
            bytes_per_float = 8  # float64 = 8 bytes
            n_points = data_shape[0]
            n_dims = data_shape[1]
            memory_per_row = n_points * n_dims * bytes_per_float

            # Calculate max batch size that fits in usable memory
            max_batch_size = int(usable_memory / memory_per_row)

            # Ensure batch size is at least 1 and no more than the dataset size
            batch_size = max(1, min(max_batch_size, n_points))

            # Round to a nice number for better cache performance
            if batch_size > 1000:
                batch_size = round(batch_size / 1000) * 1000
            elif batch_size > 100:
                batch_size = round(batch_size / 100) * 100
            elif batch_size > 10:
                batch_size = round(batch_size / 10) * 10

            return batch_size

        except (ImportError, AttributeError):
            # Fallback if psutil is not available or fails
            # A conservative default batch size
            return min(1000, data_shape[0])

    def _fit_kde(self, bandwidths, kernel_type, adaptive):
        """
        Fit KDE model using specified bandwidths and kernel. Optimized for memory
        efficiency by processing data in batches to avoid large intermediate arrays.

        Parameters:
            bandwidths (Union[np.ndarray, List[np.ndarray]]): Bandwidth matrix(ces) to use
            kernel_type (str): Type of kernel function
            adaptive (bool): Whether to use adaptive (local) bandwidths

        Returns:
            np.ndarray: Density estimates for each point
        """
        # Determine optimal batch size based on available memory
        batch_size = self._determine_optimal_batch_size(self.data.shape)

        # Initialize density array
        densities = np.zeros(len(self.data))

        # Process data in batches
        for i in range(0, len(self.data), batch_size):
            end_idx = min(i + batch_size, len(self.data))
            current_batch = slice(i, end_idx)

            # Get current batch of points
            batch_points = self.data[current_batch]

            if adaptive:
                # For adaptive bandwidth, process each point individually 
                # but vectorize the calculation against all data points
                for j, point_idx in enumerate(range(i, end_idx)):
                    point = batch_points[j]
                    diff = point.reshape(1, -1) - self.data  # shape (n_data, n_dims)

                    # Get the bandwidth for this point
                    inv_bandwidth = inv(bandwidths[point_idx])
                    d = diff.shape[1]  # dimensionality

                    # Calculate quadratic form for all data points
                    quad_form = np.sum(diff @ inv_bandwidth * diff, axis=1)

                    if kernel_type == 'gaussian':
                        coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
                        kernel_values = coefficient * np.exp(-0.5 * quad_form)

                    elif kernel_type == 'epanechnikov':
                        coefficient = (d + 2) / (2 * np.pi * det(inv_bandwidth) ** 0.5)
                        kernel_values = coefficient * np.where(quad_form <= 1, (1 - quad_form), 0)

                    elif kernel_type == 'laplacian':
                        coefficient = 1 / (2 * np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
                        kernel_values = coefficient * np.exp(-np.sqrt(quad_form))

                    else:
                        # Fallback for any other kernel types
                        kernel_values = np.array([
                            self._kernel_function(point - self.data[k], inv_bandwidth, kernel_type) 
                            for k in range(len(self.data))
                        ])

                    densities[point_idx] = np.mean(kernel_values)

            else:
                # For global bandwidth
                inv_bandwidth = inv(bandwidths)
                d = self.data.shape[1]  # dimensionality

                # Pre-compute coefficients based on kernel type
                if kernel_type == 'gaussian':
                    coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
                elif kernel_type == 'epanechnikov':
                    coefficient = (d + 2) / (2 * np.pi * det(inv_bandwidth) ** 0.5)
                elif kernel_type == 'laplacian':
                    coefficient = 1 / (2 * np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))

                # Process each point in the batch
                for j, point_idx in enumerate(range(i, end_idx)):
                    point = batch_points[j]
                    diff = point.reshape(1, -1) - self.data  # shape (n_data, n_dims)

                    # Compute quadratic form for all differences at once
                    quad_form = np.sum(diff @ inv_bandwidth * diff, axis=1)

                    if kernel_type == 'gaussian':
                        kernel_values = coefficient * np.exp(-0.5 * quad_form)

                    elif kernel_type == 'epanechnikov':
                        kernel_values = coefficient * np.where(quad_form <= 1, (1 - quad_form), 0)

                    elif kernel_type == 'laplacian':
                        kernel_values = coefficient * np.exp(-np.sqrt(quad_form))

                    else:
                        # Use standard kernel function for other types
                        kernel_values = np.array([
                            self._kernel_function(point - self.data[k], inv_bandwidth, kernel_type)
                            for k in range(len(self.data))
                        ])

                    densities[point_idx] = np.mean(kernel_values)

        return densities

    def _bayesian_optimization(self, use_global):
        """
        Optimize KDE parameters using Bayesian optimization.

        Parameters:
            use_global (bool): Whether to use global bandwidth optimization

        Returns:
            tuple: (best_parameters, best_objective_value)
                - best_parameters: List of optimized parameter values
                - best_objective_value: Final objective function value
        """
        # Ensure kernel_types is not empty
        if not self.kernel_types:
            self.kernel_types = ['gaussian']  # Default to gaussian if empty

        if use_global:
            search_space = [
                Categorical(list(range(len(self.kernel_types))), name='kernel'),
                Real(0.1, 2.0, name='scaling_factor')
            ]
            n_iter = self.n_iter  # Use full number of iterations for global bandwidth
        else:
            search_space = [
                Integer(self.k_limits[0], self.k_limits[1], name='k'),
                Categorical(list(range(len(self.kernel_types))), name='kernel'),
                Real(0.1, 2.0, name='scaling_factor')
            ]
            # For local bandwidth, limit iterations based on k range
            n_iter = min(self.n_iter, self.k_limits[1] - self.k_limits[0] + 1)

        @use_named_args(search_space)
        def objective(**params):
            kernel_index = params['kernel']
            kernel_type = self.kernel_types[kernel_index]
            self.best_scaling_factor = params.get('scaling_factor', 1.0)

            try:
                if use_global:
                    global_bandwidth = self._calculate_global_bandwidth()
                    densities = self._fit_kde(global_bandwidth, kernel_type, adaptive=False)
                else:
                    local_bandwidths = self._calculate_local_bandwidths(k=int(params['k']))
                    densities = self._fit_kde(local_bandwidths, kernel_type, adaptive=True)

                return -np.mean(np.log(np.maximum(densities, 1e-10)))
            except ValueError:
                return np.inf

        with tqdm(total=n_iter, desc="Bayesian Optimization") as pbar:
            def tqdm_callback(res):
                pbar.update(1)
                current_iter = len(self.optimization_progress) + 1
                self.optimization_progress.append({
                    'iteration': current_iter,
                    'best_objective': -np.min(res.func_vals),
                    'current_objective': -res.func_vals[-1]
                })

            result = gp_minimize(
                func=objective,
                dimensions=search_space,
                n_calls=n_iter,
                random_state=42,
                callback=[tqdm_callback],
                n_random_starts=n_iter
            )

        if use_global:
            self.best_k = None
            self.best_kernel = self.kernel_types[result.x[0]]
            self.best_scaling_factor = result.x[1]
        else:
            self.best_k = result.x[0]
            self.best_kernel = self.kernel_types[result.x[1]]
            self.best_scaling_factor = result.x[2]

        self.optimization_result = OptimizationResult(
            method_used="global_bandwidth" if use_global else "local_bandwidth",
            kernel_type=self.best_kernel,
            k=self.best_k,
            scaling_factor=self.best_scaling_factor
        )

        self._log_iterations(result)

        return result.x, -result.fun

    def _log_iterations(self, result):
        """
        Log parameters and objective values for each optimization iteration.

        Parameters:
            result: Optimization result object from gp_minimize

        Returns:
            None
        """
        iteration_data = []

        if self.method_used == 'global_bandwidth':
            for i, (params, value) in enumerate(zip(result.x_iters, result.func_vals)):
                kernel_index, scaling_factor = params
                kernel_type = self.kernel_types[kernel_index]
                iteration_data.append({
                    'iteration': i + 1,
                    'kernel': kernel_type,
                    'scaling_factor': scaling_factor,
                    'objective_value': value
                })
        else:
            for i, (params, value) in enumerate(zip(result.x_iters, result.func_vals)):
                k, kernel_index, scaling_factor = params
                kernel_type = self.kernel_types[kernel_index]
                iteration_data.append({
                    'iteration': i + 1,
                    'k': k,
                    'kernel': kernel_type,
                    'scaling_factor': scaling_factor,
                    'objective_value': value
                })

        self.iteration_results = pd.DataFrame(iteration_data)
        self.iteration_results = self.iteration_results.sort_values(by='objective_value', ascending=True).reset_index(drop=True)

    def fit(self, method='local_bandwidth'):
        """
        Fit the KDE model using specified method.

        Parameters:
            method (str): Method to use ('local_bandwidth', 'global_bandwidth', 'scott', 'silverman')

        Returns:
            None
        """
        self.method_used = method

        if method == 'local_bandwidth':
            best_params, _ = self._bayesian_optimization(use_global=False)
            local_bandwidths = self._calculate_local_bandwidths(k=int(best_params[0]))
            kernel_type = self.kernel_types[best_params[1]]
            self.point_densities = self._fit_kde(local_bandwidths, kernel_type, adaptive=True)
            self.best_kernel = kernel_type
            self.best_bandwidth = local_bandwidths

        elif method == 'global_bandwidth':
            best_params, _ = self._bayesian_optimization(use_global=True)
            global_bandwidth = self._calculate_global_bandwidth()
            kernel_type = self.kernel_types[best_params[0]]
            self.point_densities = self._fit_kde(global_bandwidth, kernel_type, adaptive=False)
            self.best_kernel = kernel_type
            self.best_bandwidth = global_bandwidth

        elif method in ['scott', 'silverman']:
            global_bandwidth = self._calculate_global_bandwidth(method=method)
            self.point_densities = self._fit_kde(global_bandwidth, kernel_type='gaussian', adaptive=False)
            self.best_kernel = 'gaussian'
            self.best_bandwidth = global_bandwidth

            self.optimization_result = OptimizationResult(
                method_used=method,
                kernel_type=self.best_kernel,
                k=None,
                scaling_factor=self.best_scaling_factor
            )
        else:
            raise ValueError("Invalid method. Choose from 'local_bandwidth', 'global_bandwidth', 'scott', or 'silverman'.")

    def calculate_grid_with_margin(self, resolution=50, buffer_fraction=0.05):
        """
        Calculate grid points with margin for density estimation.

        Parameters:
            resolution (int): Number of points along each axis for grid
            buffer_fraction (float): Fraction of data range to add as buffer

        Returns:
            tuple: (xx, yy, boundaries)
                - xx (np.ndarray): X coordinates of grid points
                - yy (np.ndarray): Y coordinates of grid points
                - boundaries (tuple): (x_min, x_max, y_min, y_max)
        """
        if (self.stored_grid is not None and 
            self.stored_grid_resolution == resolution and 
            self.stored_grid_buffer == buffer_fraction):
            return self.stored_grid[0], self.stored_grid[1], self.stored_grid_boundaries

        x_min, x_max = self.data[:, 0].min(), self.data[:, 0].max()
        y_min, y_max = self.data[:, 1].min(), self.data[:, 1].max()

        x_range = x_max - x_min
        y_range = y_max - y_min

        if self.best_bandwidth is not None:
            if isinstance(self.best_bandwidth, (int, float)):
                h_x = h_y = self.best_bandwidth
            elif isinstance(self.best_bandwidth, np.ndarray):
                h_x, h_y = np.sqrt(np.diag(self.best_bandwidth))
            elif isinstance(self.best_bandwidth, list):
                h_x = max(np.sqrt(np.diag(bw)[0]) for bw in self.best_bandwidth)
                h_y = max(np.sqrt(np.diag(bw)[1]) for bw in self.best_bandwidth)
            else:
                raise ValueError("Unsupported bandwidth type.")
        else:
            h_x = h_y = 0

        margin_x = 3 * h_x + buffer_fraction * x_range
        margin_y = 3 * h_y + buffer_fraction * y_range

        x_min -= margin_x
        x_max += margin_x
        y_min -= margin_y
        y_max += margin_y

        xx, yy = np.meshgrid(
            np.linspace(x_min, x_max, resolution),
            np.linspace(y_min, y_max, resolution)
        )

        self.stored_grid = (xx, yy)
        self.stored_grid_resolution = resolution
        self.stored_grid_buffer = buffer_fraction
        self.stored_grid_boundaries = (x_min, x_max, y_min, y_max)

        return xx, yy, (x_min, x_max, y_min, y_max)

    def calculate_grid_densities(self, resolution=50, buffer_fraction=0.05):
        """
        Compute density estimates on a grid with vectorized operations for speed.
        Optimized for all kernel types with progress bar support.

        Parameters:
            resolution (int): Number of points along each axis for grid
            buffer_fraction (float): Fraction of data range to add as buffer

        Returns:
            np.ndarray: Grid of density estimates, shape (resolution, resolution)
        """
        if self.best_kernel is None or self.best_bandwidth is None:
            raise RuntimeError("Model must be fit before calculating grid densities.")

        # Get grid coordinates
        xx, yy, _ = self.calculate_grid_with_margin(resolution=resolution, buffer_fraction=buffer_fraction)
        grid_points = np.column_stack((xx.ravel(), yy.ravel()))

        # Set up progress bar
        pbar = tqdm(total=len(grid_points), desc="Estimating grid density values", unit="point")

        if isinstance(self.best_bandwidth, list):
            # For local bandwidth - process in batches for memory efficiency
            batch_size = 100  # Adjust based on available memory
            densities = []

            for i in range(0, len(grid_points), batch_size):
                batch = grid_points[i:i + batch_size]
                batch_densities = []

                # Reshape for broadcasting
                batch_points = batch.reshape(-1, 1, 2)  # (batch, 1, 2)
                data_points = self.data.reshape(1, -1, 2)  # (1, n_data, 2)

                # Calculate differences for all points at once
                diffs = batch_points - data_points  # (batch, n_data, 2)

                for j in range(len(self.data)):
                    diff = diffs[:, j, :]  # (batch, 2)
                    inv_bandwidth = inv(self.best_bandwidth[j])

                    # Compute quadratic form for all differences at once
                    quad_form = np.einsum('bi,ij,bj->b', diff, inv_bandwidth, diff)

                    if self.best_kernel == 'gaussian':
                        coefficient = 1 / (np.sqrt((2 * np.pi) ** 2 * det(inv_bandwidth)))
                        kernel_values = coefficient * np.exp(-0.5 * quad_form)

                    elif self.best_kernel == 'epanechnikov':
                        coefficient = (2 + 2) / (2 * np.pi * det(inv_bandwidth) ** 0.5)  # d=2 for 2D data
                        kernel_values = coefficient * np.where(quad_form <= 1, (1 - quad_form), 0)

                    elif self.best_kernel == 'laplacian':
                        coefficient = 1 / (2 * np.sqrt((2 * np.pi) ** 2 * det(inv_bandwidth)))
                        kernel_values = coefficient * np.exp(-np.sqrt(quad_form))

                    else:
                        # Use standard kernel function for other types
                        kernel_values = np.array([
                            self._kernel_function(d, inv_bandwidth, self.best_kernel)
                            for d in diff
                        ])

                    batch_densities.append(kernel_values)

                # Average across all data points
                batch_result = np.mean(np.array(batch_densities), axis=0)
                densities.extend(batch_result)
                pbar.update(len(batch))

        else:
            # For global bandwidth - fully vectorized
            inv_bandwidth = inv(self.best_bandwidth)
            batch_size = 100
            densities = []
            d = 2  # dimensionality (2D data)

            # Pre-compute coefficients based on kernel type
            if self.best_kernel == 'gaussian':
                coefficient = 1 / (np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))
            elif self.best_kernel == 'epanechnikov':
                coefficient = (d + 2) / (2 * np.pi * det(inv_bandwidth) ** 0.5)
            elif self.best_kernel == 'laplacian':
                coefficient = 1 / (2 * np.sqrt((2 * np.pi) ** d * det(inv_bandwidth)))

            for i in range(0, len(grid_points), batch_size):
                batch = grid_points[i:i + batch_size]

                # Reshape for broadcasting
                batch_points = batch.reshape(-1, 1, 2)  # (batch, 1, 2)
                data_points = self.data.reshape(1, -1, 2)  # (1, n_data, 2)

                # Calculate all differences at once
                diffs = batch_points - data_points  # (batch, n_data, 2)

                # Initialize array for kernel values
                kernel_values = np.zeros((len(batch), len(self.data)))

                for b in range(len(batch)):
                    # Compute quadratic form for each grid point against all data points
                    quad_form = np.einsum('ni,ij,nj->n', 
                                        diffs[b], inv_bandwidth, diffs[b])

                    if self.best_kernel == 'gaussian':
                        kernel_values[b] = coefficient * np.exp(-0.5 * quad_form)

                    elif self.best_kernel == 'epanechnikov':
                        kernel_values[b] = coefficient * np.where(quad_form <= 1, (1 - quad_form), 0)

                    elif self.best_kernel == 'laplacian':
                        kernel_values[b] = coefficient * np.exp(-np.sqrt(quad_form))

                    else:
                        # Use standard kernel function for other types
                        kernel_values[b] = np.array([
                            self._kernel_function(d, inv_bandwidth, self.best_kernel)
                            for d in diffs[b]
                        ])

                batch_result = np.mean(kernel_values, axis=1)
                densities.extend(batch_result)
                pbar.update(len(batch))

        pbar.close()

        # Reshape to grid - same format as original
        grid_densities = np.array(densities).reshape(xx.shape)
        self.grid_densities = grid_densities
        self.stored_grid_resolution = resolution
        self.stored_grid_buffer = buffer_fraction

        return grid_densities

    def _process_point_local(self, point, bandwidths, kernel_type, data):
        """Helper function for parallel processing with local bandwidth."""
        return np.mean([
            self._kernel_function(point - data[i], inv(bandwidths[i]), kernel_type)
            for i in range(len(data))
        ])

    def _process_point_global(self, point, inv_bandwidth, kernel_type, data):
        """Helper function for parallel processing with global bandwidth."""
        return np.mean(self._kernel_function(data - point, inv_bandwidth, kernel_type))

    def plot_results(self, vmin=0, resolution=50, buffer_fraction=0.05):
        """
        Visualize KDE results with point and density plots.

        Parameters:
            vmin (float): Minimum density value for visualization
            resolution (int): Number of points along each axis for grid
            buffer_fraction (float): Fraction of data range to add as buffer

        Returns:
            None
        """
        if self.point_densities is None:
            raise RuntimeError("Model must be fit before plotting results.")

        # Compute grid densities
        grid_densities = self.get_grid_densities(resolution=resolution, buffer_fraction=buffer_fraction)
        xx, yy, (x_min, x_max, y_min, y_max) = self.calculate_grid_with_margin(
            resolution=resolution, buffer_fraction=buffer_fraction
        )
        point_densities = self.get_point_densities()

        # Create figure with two subplots
        fig, ax = plt.subplots(1, 2, figsize=(14, 6))

        # Method display mapping
        method_display = {
            'global_bandwidth': 'Global Bandwidth',
            'local_bandwidth': 'Local Bandwidth',
            'scott': "Scott's Rule Bandwidth",
            'silverman': "Silverman's Rule Bandwidth"
        }.get(self.method_used, self.method_used)

        # Construct detailed method information
        method_info = method_display
        if self.best_kernel:
            method_info += f", {self.best_kernel} kernel"
        if hasattr(self, 'best_k') and self.best_k:
            method_info += f", k={self.best_k}"
        if hasattr(self, 'best_scaling_factor') and self.best_scaling_factor is not None:
            method_info += f", scaling factor={self.best_scaling_factor:.2f}"

        # Point density plot
        point_min = np.min(point_densities)
        point_max = np.max(point_densities)
        sc = ax[0].scatter(self.data[:, 0], self.data[:, 1], 
                          c=point_densities, 
                          cmap='viridis', 
                          s=10,
                          vmin=point_min,
                          vmax=point_max)
        plt.colorbar(sc, ax=ax[0], label='KDE')
        ax[0].set_title(f'Point Densities\n({method_info})')
        ax[0].set_xlabel('X')
        ax[0].set_ylabel('Y')
        ax[0].set_xlim(x_min, x_max)
        ax[0].set_ylim(y_min, y_max)

        # Robust contour level generation
        grid_min = np.min(grid_densities)
        grid_max = np.max(grid_densities)

        # Ensure unique, increasing levels
        if grid_min == grid_max:
            # If all densities are the same, create a small range
            levels = [grid_min, grid_min * 1.1]
        else:
            # Generate levels ensuring they are strictly increasing
            levels = np.unique(np.linspace(grid_min, grid_max, num=20))

            # Add a small epsilon to ensure levels are strictly increasing
            if len(levels) < 2:
                levels = [grid_min, grid_min * 1.1]
            elif len(levels) == 2 and levels[0] == levels[1]:
                levels[1] = levels[0] * 1.1

        # Contour plot with robust level generation
        contour = ax[1].contourf(xx, yy, grid_densities, 
                                levels=levels,
                                cmap='viridis', 
                                alpha=0.6)
        plt.colorbar(contour, ax=ax[1], label='KDE')
        ax[1].set_title('Density Contours')
        ax[1].set_xlabel('X')
        ax[1].set_ylabel('Y')
        ax[1].set_xlim(x_min, x_max)
        ax[1].set_ylim(y_min, y_max)

        plt.tight_layout()

        # Save figure if save path is specified
        if self.save_path:
            plt.savefig(self.save_path, dpi=300, bbox_inches='tight')

        plt.show()
    
    def plot_optimization_progress(self, save_path=None):
        """
        Plot the optimization progress over iterations.

        Parameters:
            save_path (str, optional): Path to save the plot

        Returns:
            None
        """
        if not self.optimization_progress:
            raise RuntimeError("No optimization progress available. Run fit() first.")

        plt.figure(figsize=(12, 8))

        iterations = [prog['iteration'] for prog in self.optimization_progress]
        best_objectives = [prog['best_objective'] for prog in self.optimization_progress]
        current_objectives = [prog['current_objective'] for prog in self.optimization_progress]

        plt.plot(iterations, best_objectives, 'b-', label='Best value')
        plt.scatter(iterations, current_objectives, c='red', alpha=0.3, s=30, label='Evaluated points')

        plt.xlabel('Iteration')
        plt.ylabel('Objective Value')

        method = "Global" if self.method_used == "global_bandwidth" else "Local"
        final_score = best_objectives[-1]
        plt.title(f'{method} Bandwidth Optimization Progress\n'
                 f'Final parameters: {self.best_kernel} kernel'
                 f'{", k=" + str(self.best_k) if self.best_k else ""}'
                 f', scaling={self.best_scaling_factor:.3f}\n'
                 f'Final objective value: {final_score:.4f}')

        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend(loc='lower right')
        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

    def get_point_densities(self):
        """
        Get density estimates for input points.

        Parameters:
            None

        Returns:
            np.ndarray: Density estimates for each input point
        """
        return self.point_densities

    def get_grid_densities(self, resolution=50, buffer_fraction=0.05):
        """
        Get or compute density estimates on a grid.

        Parameters:
            resolution (int): Number of points along each axis for grid
            buffer_fraction (float): Fraction of data range to add as buffer

        Returns:
            np.ndarray: Grid of density estimates
        """
        if self.grid_densities is None:
            self.calculate_grid_densities(resolution=resolution, buffer_fraction=buffer_fraction)
        return self.grid_densities

    def get_optimization_report(self):
        """
        Get report of optimization parameters.

        Parameters:
            None

        Returns:
            OptimizationResult: Named tuple containing optimization results
        """
        if self.optimization_result is None:
            raise RuntimeError("Model must be fit before generating a report.")
        return self.optimization_result
    
    def print_optimization_report(self):
        """
        Print formatted report of optimization parameters.

        Parameters:
            None

        Returns:
            None
        """
        if self.optimization_result is None:
            raise RuntimeError("Model must be fit before printing a report.")

        print("Optimization Report:")
        print(f"- Method Used: {self.optimization_result.method_used}")
        print(f"- Kernel Type: {self.optimization_result.kernel_type}")
        print(f"- k: {self.optimization_result.k}")
        print(f"- Scaling Factor: {self.optimization_result.scaling_factor:.4f}")
    
    def get_iteration_results(self):
        """
        Get DataFrame of iteration results.

        Parameters:
            None

        Returns:
            pd.DataFrame: Results from each optimization iteration
        """
        if self.iteration_results is None:
            raise RuntimeError("Iteration results are not available.")
        return self.iteration_results
    
    def save_iteration_results(self, file_path):
        """
        Save iteration results to CSV file.

        Parameters:
            file_path (str): Path to save CSV file

        Returns:
            None
        """
        if not hasattr(self, 'iteration_results'):
            raise RuntimeError("Iteration results are not available.")
        self.iteration_results.to_csv(file_path, index=False)








import numpy as np
import pandas as pd
from scipy.spatial import KDTree
import matplotlib.pyplot as plt
from shapely.geometry import Point
from shapely.geometry.polygon import Polygon
import networkx as nx
import warnings
import copy
from typing import List, Tuple, Dict, Optional, Union, Any
from tqdm import tqdm

warnings.filterwarnings('ignore')
np.random.seed(42)

class ClusterDC:
    """
    Density-Contour Clustering with Automatic Peak Detection.
    
    This class implements a density-based clustering algorithm that uses kernel density
    estimation (KDE) and contour analysis to identify clusters in 2D data. The algorithm:
    1. Estimates density using KDE
    2. Creates density contours
    3. Analyzes contour hierarchies
    4. Identifies density peaks
    5. Determines cluster assignments based on density connectivity
    
    Key Features:
    - Multiple initialization methods (raw data or KDE object)
    - Automatic selection of number of clusters using gap analysis
    - Manual specification of desired number of clusters
    - Hierarchical density-based clustering
    - Visualization tools for clustering results
    - Support for different density estimation methods
    
    The algorithm is particularly effective for:
    - Identifying clusters of varying shapes and sizes
    - Handling noise in the data
    - Detecting natural hierarchies in cluster structure
    - Working with non-linear cluster boundaries
    """
    def __init__(
        self,
        data: Optional[Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]] = None,
        kde_obj: Optional['KDE'] = None,
        levels: int = 50,
        min_point: int = 1,
        gap_order: Optional[Union[int, str, None]] = 1,  # Changed from selection_method
        n_clusters: Optional[int] = None,
        save_path: Optional[str] = None,
        xlabel: Optional[str] = None,
        ylabel: Optional[str] = None,
        kde_method: str = 'scott',
        kde_kernel_types: Optional[List[str]] = None,
        kde_n_iter: int = 50,
        kde_k_limits: tuple = (1, 40)
    ):
        """
        Initialize the ClusterDC algorithm.

        Parameters:
            data: Input data in one of two formats:
                - np.ndarray: shape (n_samples, 2) for 2D data
                - Tuple[np.ndarray, np.ndarray]: x and y coordinates separately
            kde_obj: Pre-fitted KDE object (alternative to raw data)
            levels: Number of density contour levels
            min_point: Minimum points required for a valid cluster
            gap_order: Method to select number of clusters:
                - int: Gap order (1 for first gap, 2 for second, etc.)
                - 'max_clusters': Use all positive separability points
                - 'all': Return all possible clusterings
                - None: Use user-specified n_clusters
            n_clusters: Exact number of clusters (used when gap_order is None)
            save_path: Path to save visualizations
            xlabel: Label for x-axis in plots
            ylabel: Label for y-axis in plots
            kde_method: Method for KDE ('scott', 'silverman', 'local_bandwidth', 'global_bandwidth')
            kde_kernel_types: List of kernel functions for KDE
            kde_n_iter: Number of iterations for KDE optimization
            kde_k_limits: Range of k values for local bandwidth as percentage

        Returns:
            None

        Raises:
            ValueError: If neither data nor kde_obj is provided
            ValueError: If input validation fails
        """
        self.levels = levels
        self.min_point = min_point
        self.gap_order = gap_order  # Changed from selection_method
        self.n_clusters = n_clusters
        self.save_path = save_path
        self.xlabel = xlabel if xlabel is not None else 'X'
        self.ylabel = ylabel if ylabel is not None else 'Y'
        self.cluster_assignments = None
        self._temp_max_flow = None
        self._temp_dens_list = None

        # Validate inputs
        if gap_order is None and n_clusters is None:
            raise ValueError("If gap_order is None, n_clusters must be specified")
        if gap_order is not None and n_clusters is not None:
            raise ValueError("Cannot specify both gap_order and n_clusters")
        if n_clusters is not None and n_clusters < 1:
            raise ValueError("n_clusters must be positive")
        if isinstance(gap_order, int) and gap_order < 1:
            raise ValueError("When using gap order (int), value must be positive")
        elif isinstance(gap_order, str) and gap_order not in ['max_clusters', 'all']:
            raise ValueError("When using string gap_order, must be 'max_clusters' or 'all'")

        if kde_obj is not None:
            self._initialize_from_kde(kde_obj)
        elif data is not None:
            self._initialize_from_data(data, kde_method, kde_kernel_types, kde_n_iter, kde_k_limits)
        else:
            raise ValueError("Either data or kde_obj must be provided")

    def _initialize_from_kde(self, kde_obj: 'KDE'):
        """
        Initialize using a fitted KDE object.

        Parameters:
            kde_obj: Pre-fitted KDE object containing density estimates

        Returns:
            None

        Raises:
            ValueError: If KDE object is not fitted or data is not 2D
        """
        if not hasattr(kde_obj, 'point_densities') or kde_obj.point_densities is None:
            raise ValueError("KDE object must be fitted before use")

        # Get the data points from KDE object
        if kde_obj.data.shape[1] != 2:
            raise ValueError("KDE data must be 2-dimensional")
            
        self.x_points = kde_obj.data[:, 0]
        self.y_points = kde_obj.data[:, 1]
        self.xy = kde_obj.data
        
        # Get the densities and grid from KDE object
        self.point_densities = kde_obj.get_point_densities()
        self.grid_densities = kde_obj.get_grid_densities()
        self.xx, self.yy, _ = kde_obj.calculate_grid_with_margin()

    def _initialize_from_data(
        self,
        data: Union[np.ndarray, Tuple[np.ndarray, np.ndarray]],
        kde_method: str,
        kde_kernel_types: Optional[List[str]],
        kde_n_iter: int,
        kde_k_limits: tuple
    ):
        """
        Initialize using raw data and KDE parameters.

        Parameters:
            data: Input data as array or tuple of arrays
            kde_method: Method for density estimation
            kde_kernel_types: List of kernel functions
            kde_n_iter: Number of optimization iterations
            kde_k_limits: Range for k nearest neighbors

        Returns:
            None

        Raises:
            ValueError: If data is not 2D
        """
        # Handle different input formats
        if isinstance(data, tuple):
            self.x_points, self.y_points = data
            self.xy = np.column_stack((self.x_points, self.y_points))
        else:
            if data.shape[1] != 2:
                raise ValueError("Data must be 2-dimensional")
            self.xy = data
            self.x_points = data[:, 0]
            self.y_points = data[:, 1]

        # Calculate KDE
        kde = KDE(
            data=self.xy,
            k_limits=kde_k_limits,
            kernel_types=kde_kernel_types,
            n_iter=kde_n_iter
        )
        kde.fit(method=kde_method)
        
        # Get densities and grid
        self.grid_densities = kde.get_grid_densities()
        self.point_densities = kde.get_point_densities()
        self.xx, self.yy, _ = kde.calculate_grid_with_margin()

    def create_contours(self) -> Tuple[List, List, List]:
        """
        Create density contours from the grid density estimation.
        
        Uses matplotlib's contour function to create density level sets,
        which are then converted to polygons for further analysis.

        Returns:
            Tuple containing:
            - List of polygon coordinates (x, y pairs)
            - List of density values for each contour
            - List of contour level indices
        """
        fig, ax = plt.subplots(figsize=(10, 10))
        cset = ax.contour(self.xx, self.yy, self.grid_densities, levels=self.levels)
        plt.close()
        
        polygons, density_values, density_contours = [], [], []
        for j, level in enumerate(cset.levels):
            for seg in cset.allsegs[j]:
                polygons.append([seg[:, 0], seg[:, 1]])
                density_values.append(level)
                density_contours.append(j)
        
        return polygons, density_values, density_contours

    def make_node_dic(
        self,
        polygons: List,
        density_values: List,
        density_contours: List
    ) -> Dict:
        """
        Create a hierarchical dictionary of contour nodes.
        
        Analyzes containment relationships between contours to build
        a tree structure representing the density hierarchy.
    
        Parameters:
            polygons: List of polygon coordinates
            density_values: List of density values for each contour
            density_contours: List of contour level indices
    
        Returns:
            Dictionary with hierarchical node structure where each node contains:
            - level: Contour level
            - density: Density value
            - childrens: List of child node indices
            - parent: List of parent node indices
            - token: List of assigned cluster tokens
        """
        dic_node = {
            ind: {
                'level': density_contours[ind],
                'density': density_values[ind],
                'childrens': [],
                'parent': [],
                'token': []
            } for ind in range(len(polygons))
        }
        
        for ind in range(len(polygons)):
            # Check if the polygon has valid coordinates
            if (len(polygons[ind][0]) > 0 and len(polygons[ind][1]) > 0):
                # Check if polygon is closed (first and last points match)
                if (polygons[ind][0][0] == polygons[ind][0][-1] and 
                    polygons[ind][1][0] == polygons[ind][1][-1]):
                    try:
                        poly = Polygon([
                            (polygons[ind][0][index], polygons[ind][1][index])
                            for index in range(len(polygons[ind][0]))
                        ])
    
                        for ind_sub in range(len(polygons)):
                            # Skip empty polygons or self-comparison
                            if ind_sub == ind or len(polygons[ind_sub][0]) == 0 or len(polygons[ind_sub][1]) == 0:
                                continue
                                
                            # Check if the sub-polygon is closed
                            if (polygons[ind_sub][0][0] == polygons[ind_sub][0][-1] and 
                                polygons[ind_sub][1][0] == polygons[ind_sub][1][-1]):
                                # Check level and density conditions
                                if (dic_node[ind_sub]['level'] == dic_node[ind]['level'] + 1 and 
                                    dic_node[ind_sub]['density'] > dic_node[ind]['density']):
                                    try:
                                        poly_sub = Polygon([
                                            (polygons[ind_sub][0][index], polygons[ind_sub][1][index])
                                            for index in range(len(polygons[ind_sub][0]))
                                        ])
                                        if poly.contains(poly_sub):
                                            dic_node[ind]['childrens'].append(ind_sub)
                                            dic_node[ind_sub]['parent'].append(ind)
                                    except Exception as e:
                                        print(f"Warning: Error creating sub-polygon {ind_sub}: {e}")
                                        continue
                    except Exception as e:
                        print(f"Warning: Error creating polygon {ind}: {e}")
                        continue
    
        # Check for maximum level
        max_level = max(node['level'] for node in dic_node.values())
        
        # Handle nodes at each level, from highest to lowest
        for level in range(max_level, 0, -1):
            list_node_level = [
                node for node in dic_node 
                if dic_node[node]['level'] == level
            ]
            for ind_node in list_node_level:
                # Check if the polygon exists and has valid coordinates
                if (ind_node in dic_node and 
                    ind_node < len(polygons) and 
                    len(polygons[ind_node][0]) > 0 and 
                    len(polygons[ind_node][1]) > 0):
                    try:
                        poly = Polygon([
                            (polygons[ind_node][0][index], polygons[ind_node][1][index])
                            for index in range(len(polygons[ind_node][0]))
                        ])
                        for ind_node_sub in list_node_level:
                            if ind_node != ind_node_sub and ind_node_sub in dic_node:
                                # Check if the sub-polygon has valid coordinates
                                if (ind_node_sub < len(polygons) and 
                                    len(polygons[ind_node_sub][0]) > 0 and 
                                    len(polygons[ind_node_sub][1]) > 0):
                                    try:
                                        poly_sub = Polygon([
                                            (polygons[ind_node_sub][0][index], polygons[ind_node_sub][1][index])
                                            for index in range(len(polygons[ind_node_sub][0]))
                                        ])
                                        if poly.contains(poly_sub) and ind_node_sub in dic_node:
                                            del dic_node[ind_node_sub]
                                    except Exception as e:
                                        print(f"Warning: Error checking polygon containment: {e}")
                                        continue
                    except Exception as e:
                        print(f"Warning: Error creating polygon {ind_node}: {e}")
                        continue
    
        # Set up root level connections
        children = [ind for ind in dic_node if dic_node[ind]['level'] == 1]
        for child in children:
            dic_node[child]['parent'] = ['*']
        dic_node['*'] = {
            'level': 0,
            'density': 0,
            'childrens': children,
            'parent': ['**'],
            'token': []
        }
        
        return dic_node

    def find_leaves(self, dic_node: Dict) -> List:
        """
        Find leaf nodes in the contour hierarchy.
        
        Leaf nodes represent local density maxima and are potential cluster centers.

        Parameters:
            dic_node: Dictionary of nodes in the contour hierarchy

        Returns:
            List of indices for leaf nodes (nodes with no children)
        """
        return [
            ind for ind in dic_node.keys()
            if ind != '*' and not dic_node[ind]['childrens']
        ]

    def check_node_unvalid(
        self,
        ind_node: str,
        polygons: List,
        min_point: int
    ) -> bool:
        """
        Check if a node is invalid based on minimum point criterion.
        
        A node is considered invalid if it contains fewer points than
        the specified minimum.

        Parameters:
            ind_node: Index of node to check
            polygons: List of polygon coordinates
            min_point: Minimum number of points required

        Returns:
            bool: True if node is invalid, False otherwise
        """
        if ind_node == '*':
            return False
            
        poly_current = Polygon([
            (polygons[ind_node][0][index], polygons[ind_node][1][index])
            for index in range(len(polygons[ind_node][0]))
        ])
        
        nb_point_inside = sum(
            1 for point in self.xy
            if Point(point[0], point[1]).within(poly_current)
        )
        
        return nb_point_inside < min_point

    def trim_unvalid_leaves(self, dic_node: Dict, polygons: List, min_point: int) -> Dict:
        """
        Remove invalid nodes from the hierarchy.

        Invalid nodes are those containing fewer points than the minimum threshold.

        Parameters:
            dic_node: Dictionary of nodes in contour hierarchy
            polygons: List of polygon coordinates
            min_point: Minimum number of points required for validity

        Returns:
            Dict: Updated node dictionary with invalid nodes removed

        Raises:
            ValueError: If dic_node is empty or if polygons list is empty
        """
        if not dic_node:
            raise ValueError("Empty node dictionary provided")
        if not polygons:
            raise ValueError("Empty polygons list provided")

        dic_node_copy = copy.deepcopy(dic_node)

        # Identify levels present in the node dictionary
        try:
            levels = sorted(set(
                node.get('level', -1) 
                for node in dic_node_copy.values() 
                if isinstance(node, dict)
            ), reverse=True)
        except Exception as e:
            raise ValueError(f"Error identifying hierarchy levels: {str(e)}")

        for level in levels:
            # Find nodes at this specific level
            nodes_at_level = [
                node for node, node_data in dic_node_copy.items() 
                if isinstance(node_data, dict) and node_data.get('level') == level
            ]

            for node in nodes_at_level:
                try:
                    # Skip special nodes
                    if node in ['*', '**']:
                        continue

                    # Safely get the parent
                    parent_list = dic_node_copy[node].get('parent', ['*'])
                    parent = parent_list[0] if parent_list else '*'

                    # Check if the node is invalid
                    if self.check_node_unvalid(node, polygons, min_point):
                        # Update parent's children list
                        if parent in dic_node_copy and isinstance(dic_node_copy[parent], dict):
                            children = dic_node_copy[parent].get('childrens', [])
                            dic_node_copy[parent]['childrens'] = [
                                child for child in children if child != node
                            ]

                        # Remove the invalid node
                        dic_node_copy.pop(node, None)

                except Exception as e:
                    print(f"Warning: Error processing node {node}: {str(e)}")
                    continue

        return dic_node_copy

    def trim_chain_nodes(self, dic_node: Dict) -> Dict:
        """
        Remove chain nodes from the hierarchy.

        Chain nodes are those with exactly one child. These are removed to
        simplify the hierarchy while preserving the cluster structure.

        Parameters:
            dic_node: Dictionary of nodes in contour hierarchy

        Returns:
            Dict: Updated node dictionary with chain nodes removed

        Raises:
            ValueError: If dic_node is empty
        """
        if not dic_node:
            raise ValueError("Empty node dictionary provided")

        dic_node_copy = copy.deepcopy(dic_node)

        try:
            # Identify levels present in the node dictionary
            levels = sorted(set(
                node.get('level', -1) 
                for node in dic_node_copy.values() 
                if isinstance(node, dict)
            ), reverse=True)
        except Exception as e:
            raise ValueError(f"Error identifying hierarchy levels: {str(e)}")

        for level in levels:
            # Find nodes at this specific level
            nodes_at_level = [
                node for node, node_data in dic_node_copy.items() 
                if isinstance(node_data, dict) and node_data.get('level') == level
            ]

            for node in nodes_at_level:
                try:
                    # Skip special nodes
                    if node in ['*', '**']:
                        continue

                    # Safely get the parent
                    parent_list = dic_node_copy[node].get('parent', ['**'])
                    parent = parent_list[0] if parent_list else '**'

                    # Skip root-level nodes
                    if parent == '**':
                        continue

                    # Check if the node has exactly one child
                    children = dic_node_copy[node].get('childrens', [])
                    if len(children) == 1:
                        child = children[0]

                        # Update parent's children
                        if parent in dic_node_copy and isinstance(dic_node_copy[parent], dict):
                            parent_children = dic_node_copy[parent].get('childrens', [])
                            dic_node_copy[parent]['childrens'] = [
                                child if n == node else n for n in parent_children
                            ]

                        # Update child's parent
                        if child in dic_node_copy and isinstance(dic_node_copy[child], dict):
                            dic_node_copy[child]['parent'] = [parent]

                        # Remove the chain node
                        dic_node_copy.pop(node, None)

                except Exception as e:
                    print(f"Warning: Error processing node {node}: {str(e)}")
                    continue

        return dic_node_copy

    def find_closest_point(
        self,
        point: np.ndarray,
        points: np.ndarray
    ) -> Tuple[np.ndarray, int, float]:
        """
        Find the closest point in a set of points to a given point.
        
        Uses KD-tree for efficient nearest neighbor search.

        Parameters:
            point: Target point coordinates
            points: Array of points to search in

        Returns:
            Tuple containing:
            - np.ndarray: Coordinates of closest point
            - int: Index of closest point
            - float: Distance to closest point
        """
        tree = KDTree(points)
        dist, idx = tree.query(point)
        return points[idx], idx, dist

    def compute_density_peak(
        self,
        polygons: List,
        ind_peak: List
    ) -> Tuple[List, List]:
        """
        Compute density peaks for each leaf node.
        
        For each leaf node (potential cluster center), finds the point
        with maximum density within its contour.

        Parameters:
            polygons: List of polygon coordinates
            ind_peak: List of indices for leaf nodes

        Returns:
            Tuple containing:
            - List: Coordinates of density peaks
            - List: Density values at peaks
        """
        peak_center, peak_center_density = [], []
        
        for ind in ind_peak:
            poly_current = Polygon([
                (polygons[ind][0][index], polygons[ind][1][index])
                for index in range(len(polygons[ind][0]))
            ])
            
            inside_points = [
                self.point_densities[i]
                for i, point in enumerate(self.xy)
                if Point(point[0], point[1]).within(poly_current)
            ]
            
            if inside_points:
                max_density_index = np.argmax(inside_points)
                peak_center_density.append(inside_points[max_density_index])
                peak_center.append(self.xy[max_density_index])
                
        return peak_center, peak_center_density

    def create_graph(
        self,
        polygons: List,
        density_values: List,
        dic_node: Dict
    ) -> nx.Graph:
        """
        Create a graph representation of the contour hierarchy.
        
        Constructs a graph where nodes represent contours and edges
        represent containment relationships. Edge weights are based
        on density values.

        Parameters:
            polygons: List of polygon coordinates
            density_values: List of density values for each contour
            dic_node: Dictionary of nodes in contour hierarchy

        Returns:
            nx.Graph: NetworkX graph object representing the hierarchy
        """
        G = nx.Graph()

        for node in dic_node.keys():
            try:
                # Safely get the parent, with fallback to '*' or '**'
                parent_list = dic_node[node].get('parent', ['*'])
                parent = parent_list[0] if parent_list else '*'

                # Add nodes to the graph
                G.add_node(node)
                G.add_node(parent)

                # Determine edge weight
                if parent in ['**', '*']:
                    # Use random weight for root-level nodes
                    edge_weight = (0, np.random.uniform(0, 1))
                else:
                    # Use density value for non-root nodes
                    edge_weight = (density_values[parent], np.random.uniform(0, 1))

                # Add edge with weight
                G.add_edge(node, parent, weight=edge_weight)

            except (KeyError, IndexError) as e:
                # Log any problematic nodes
                print(f"Error processing node {node} in create_graph: {e}")
                # Skip this node to continue processing
                continue

        return G

    def path_max(
        self,
        G: nx.Graph,
        s: Any,
        t: Any,
        current_depth: int
    ) -> float:
        """
        Find the maximum density path between two nodes.
        
        Uses a recursive algorithm to find the path with maximum density
        between two nodes in the graph. This is used to determine cluster
        separability.

        Parameters:
            G: NetworkX graph
            s: Source node
            t: Target node
            current_depth: Current recursion depth

        Returns:
            float: Maximum density along the path
        """
        if len(list(G.edges.data())) == 1:
            return list(G.edges.data())[0][2]['weight'][0]
        if s == t:
            return 0

        weight = np.array([w[0] for u, v, w in G.edges(data='weight')])
        rand_weight = np.array([w[1] for u, v, w in G.edges(data='weight')])
        median = np.median(weight)
        
        G_k = nx.Graph()
        G_k.add_nodes_from(G)
        
        if len(np.unique(weight)) == 1:
            return median
            
        for edge in list(G.edges.data()):
            if edge[2]['weight'][0] > median:
                G_k.add_edge(edge[0], edge[1], weight=edge[2]['weight'])
            if edge[2]['weight'][0] == median:
                if edge[2]['weight'][1] >= np.random.uniform(0, 1):
                    G_k.add_edge(edge[0], edge[1], weight=edge[2]['weight'])

        if t in list(nx.node_connected_component(G_k, s)):
            return self.path_max(G_k.subgraph(nx.node_connected_component(G_k, s)), s, t, current_depth + 1)
        else:
            S = [G_k.subgraph(c) for c in nx.connected_components(G_k)]
            G_bar_k = nx.Graph()
            x, y = None, None
            
            for u, G_u in enumerate(S):
                G_bar_k.add_node(u)
                if s in list(G_u.nodes()):
                    x = u
                if t in list(G_u.nodes()):
                    y = u
                    
                for v, G_v in enumerate(S):
                    if u != v:
                        maximum_weight = None
                        maximum_link = [None, None]
                        for node_u in list(G_u.nodes()):
                            for node_v in list(G_v.nodes()):
                                if node_v in list(G.neighbors(node_u)):
                                    w = G[node_u][node_v]['weight'][0]
                                    rand_value = G[node_u][node_v]['weight'][1]
                                    if maximum_weight is None:
                                        maximum_weight = w
                                        maximum_link[0] = node_u
                                        maximum_link[1] = node_v
                                    else:
                                        if w > maximum_weight:
                                            maximum_weight = w
                                            maximum_link[0] = node_u
                                            maximum_link[1] = node_v
                        if maximum_weight is not None:
                            G_bar_k.add_edge(u, v, weight=(maximum_weight, np.random.uniform(0, 1)))
                            
            return self.path_max(G_bar_k, x, y, current_depth + 1)

    def calculate_maximum_flow(self, G, peak_center_density, ind_peak):
        """
        Calculate maximum flow between density peaks.

        This method computes the maximum density path between all pairs of peaks
        to determine cluster separability. The flows are used to calculate
        gap values that determine distinct clusters.

        Parameters:
            G (nx.Graph): NetworkX graph representing the density hierarchy.
                Nodes represent contours and edges represent containment relationships.
                Edge weights are tuples of (density_value, random_value).

            peak_center_density (List[float]): List of density values at peak centers.
                These values represent the estimated density at each potential
                cluster center.

            ind_peak (List[int]): List of indices for density peaks.
                These indices correspond to the leaf nodes in the contour hierarchy
                that represent potential cluster centers.

        Returns:
            Tuple containing:
            - List[float]: Maximum flow values between peaks.
                Each value represents the highest density path between a peak
                and all higher-density peaks.

            - np.ndarray: Array of sorted density values.
                Contains the density values at peak centers, sorted in
                descending order.

            - pd.DataFrame: DataFrame containing peak information.
                Includes columns for peak indices and their density values,
                sorted by density in descending order.

        Raises:
            ValueError: If G is empty or not a valid NetworkX graph
            ValueError: If peak_center_density is empty or contains invalid values
            ValueError: If ind_peak is empty or contains invalid indices

        Note:
            The maximum flow values are used to calculate separability scores
            that determine how distinct each cluster is. A higher flow value
            indicates a stronger connection (less separation) between peaks,
            while a lower flow value indicates better cluster separation.

        Example:
            >>> max_flow, dens_list, dens_df = self.calculate_maximum_flow(G, peak_densities, peak_indices)
            >>> print(f"Number of potential clusters: {len(max_flow)}")
            >>> print(f"Maximum density value: {dens_list[0]}")
        """
        # Create and sort density DataFrame
        dens_df = pd.DataFrame({'density': peak_center_density}, index=ind_peak)
        dens_df = dens_df.sort_values(by=['density'], ascending=False)
        dens_list = dens_df.to_numpy()
        ind_cand = dens_df.index

        # Calculate maximum flows
        max_flow = []
        for ind, i in enumerate(ind_cand):
            maximum = -np.inf
            for j in ind_cand[:ind]:
                current_depth = 0
                flow = self.path_max(G, i, j, current_depth)
                if flow > maximum:
                    maximum = flow

            # Handle case of first point or no flow found
            if maximum == -np.inf:
                if ind == 0:
                    max_flow.append(0)
                else:
                    max_flow.append(np.inf)
            else:
                max_flow.append(maximum)

        # Store results for later use
        self._max_flow = max_flow
        self._dens_list = dens_list
        self._dens_df = dens_df

        return max_flow, dens_list, dens_df

    def get_well_separated_points(self, max_flow, dens_list, dens_df, print_table=True):
        """
        Returns well-separated points from the input data based on the specified selection method.
        This function is central to the clustering process, analyzing the separability between 
        density peaks and determining cluster cutoffs.

        Parameters:
        -----------
        max_flow : list
            Maximum flow values between peaks, representing the highest density path
            between each peak and all higher-density peaks.

        dens_list : numpy.ndarray
            Array of density values at peak centers, sorted in descending order.

        dens_df : pandas.DataFrame
            DataFrame containing peak information with indices mapping to peak locations
            and density values.

        print_table : bool, default=True
            Whether to print the separability table. Set to False to delay printing.

        Returns:
        --------
        tuple
            (numpy.ndarray, pandas.DataFrame): 
            - Indices of selected peaks that will serve as cluster centers
            - Display DataFrame for later printing if print_table=False
        """
        # Calculate separability for each point
        nb_values = len(max_flow)
        separability = np.array([1 - max_flow[i]/dens_list[i][0] for i in range(nb_values)])

        # Add separability to DataFrame and sort
        dens_df['separability'] = separability
        dens_df = dens_df.sort_values(by=['separability'], ascending=False)

        if self.n_clusters is not None:
            well_separated = dens_df[dens_df['separability'] > 0]
            max_possible_clusters = len(well_separated)

            if self.n_clusters > max_possible_clusters:
                if print_table:
                    print(f"\nWarning: Requested number of clusters ({self.n_clusters}) exceeds maximum possible clusters ({max_possible_clusters})")
                    print(f"Using maximum available clusters: {max_possible_clusters}")

            return well_separated.iloc[:self.n_clusters].index.to_numpy(), None

        # Get positive separability points and add reference point
        well_separated = dens_df[dens_df['separability'] > 0].copy()
        well_separated.loc['*',:] = [0, 0]

        # Calculate gaps
        well_separated['gap'] = -well_separated.separability.diff()

        # Create a display DataFrame
        display_df = well_separated.copy()
        display_df = display_df.reset_index()

        # Find gap order (excluding first row which has NaN gap)
        gaps = display_df['gap'].iloc[1:].to_numpy()
        gap_order_indices = np.argsort(-gaps)  # Sort in descending order

        # Add gap_order column for display
        display_df['gap_order'] = None
        for order, idx in enumerate(gap_order_indices, 1):
            display_df.loc[idx + 1, 'gap_order'] = order

        # Display the formatted output if requested
        if print_table:
            print("\nSeparability and gaps:")
            display_output = display_df[['separability', 'gap', 'gap_order']].fillna('--').round(6)
            print(display_output.to_string())

        if isinstance(self.gap_order, int):
            if self.gap_order <= len(gap_order_indices):
                # Get the position of the nth largest gap
                cut_position = gap_order_indices[self.gap_order - 1] + 1
                # Return points up to and including the cut position
                return well_separated.iloc[:cut_position + 1].index.to_numpy()[:-1], display_df

            else:
                return well_separated.index.to_numpy()[:-1], display_df  # Exclude reference point

        elif self.gap_order == 'max_clusters':
            return well_separated.index.to_numpy()[:-1], display_df  # Exclude reference point

        elif self.gap_order == 'all':
            return well_separated.index.to_numpy()[:-1], display_df  # Exclude reference point

        else:
            raise ValueError("Invalid gap_order parameter. Must be int, 'max_clusters', or 'all'.")
   
    
    def clean_dic_after_choice(self, ind_selection, dic_node):
        """
        Clean and update node dictionary after cluster selection.

        This method updates the hierarchical structure to reflect the selected clusters.
        It maintains the token hierarchy and ensures proper parent-child relationships.

        Parameters:
            ind_selection: List of indices for selected cluster centers
            dic_node: Dictionary representing the node hierarchy

        Returns:
            dict: Updated node dictionary reflecting cluster selection
        """
        dic_node_copy = copy.deepcopy(dic_node)

        # First, add tokens to leaves
        list_leaves = self.find_leaves(dic_node_copy)
        for leaf in list_leaves:
            if leaf in ind_selection:
                dic_node_copy[leaf]['token'] = [leaf]
            else:
                dic_node_copy[leaf]['token'] = []

        # Process nodes from highest level to root
        max_level = max(
            node.get('level', -1) 
            for node in dic_node_copy.values() 
            if isinstance(node, dict)
        )

        for level in range(max_level, -1, -1):
            nodes_at_level = [
                node for node, node_data in dic_node_copy.items() 
                if isinstance(node_data, dict) and node_data.get('level') == level
            ]

            for node in nodes_at_level:
                if node == '*':
                    continue

                parent_list = dic_node_copy[node].get('parent', ['*'])
                parent = parent_list[0] if parent_list else '*'

                tokens = dic_node_copy[node].get('token', [])
                valid_tokens = [t for t in tokens if t in ind_selection]

                dic_node_copy[node]['token'] = valid_tokens

                if valid_tokens and parent in dic_node_copy:
                    dic_node_copy[parent]['token'].extend(valid_tokens)

        # Add all selected indices to root node's token
        if '*' in dic_node_copy:
            dic_node_copy['*']['token'] = list(ind_selection)

        return dic_node_copy


    def get_point_polygon(
        self,
        polygons: List,
        dic_node: Dict
    ) -> Dict:
        """
        Create mapping of points to polygon nodes.
        
        For each node, identifies which points lie within its polygon.

        Parameters:
            polygons: List of polygon coordinates
            dic_node: Dictionary of nodes in contour hierarchy

        Returns:
            Dict: Mapping of node indices to lists of contained points
        """
        dic_point_node = {ind: [] for ind in dic_node.keys()}

        # Create KDTree once for all points
        tree = KDTree(self.xy)

        for ind_node in dic_node.keys():
            if ind_node != '*':
                try:
                    # Convert polygon to shapely Polygon once
                    polygon_coords = [(polygons[ind_node][0][i], polygons[ind_node][1][i]) 
                                    for i in range(len(polygons[ind_node][0]))]
                    poly_current = Polygon(polygon_coords)

                    # Get polygon bounds for quick filtering
                    minx, miny, maxx, maxy = poly_current.bounds

                    # Query KDTree for points in bounding box
                    potential_points = tree.query_ball_point(
                        [(minx + maxx)/2, (miny + maxy)/2],
                        r=max(maxx - minx, maxy - miny)
                    )

                    # Filter points using shapely
                    for k in potential_points:
                        point = self.xy[k]
                        if Point(point[0], point[1]).within(poly_current):
                            dic_point_node[ind_node].append([k, point, self.point_densities[k]])

                except Exception:
                    continue

        # Add all points to root node more efficiently
        dic_point_node['*'] = [[k, point, self.point_densities[k]] 
                              for k, point in enumerate(self.xy)]

        return dic_point_node
    
    def find_closest_point_vectorized(point, points):
        """Vectorized version of closest point finding"""
        distances = np.sum((points - point) ** 2, axis=1)
        idx = np.argmin(distances)
        return points[idx], idx, np.sqrt(distances[idx])

    def assign_points(self, xy, dic_point_node, dic_node):
        """
        Assigns cluster labels to points using a hierarchical density-based approach.
        This function implements a top-down point assignment strategy, processing nodes
        from highest to lowest density levels and ensuring connectivity within clusters.

        Parameters:
        -----------
        xy : numpy.ndarray
            Array of point coordinates with shape (n_points, 2)

        dic_point_node : dict
            Dictionary mapping node indices to lists of contained points, where each point
            is represented as [index, coordinates, density]

        dic_node : dict
            Hierarchical structure of density contours where each node contains:
            - level: Contour level in hierarchy
            - childrens: List of child node indices
            - token: List of cluster assignments
            - parent: Parent node index

        Returns:
        --------
        numpy.ndarray
            Array of cluster assignments for each point, numbered from 1 to n_clusters

        Algorithm Steps:
        ---------------
        1. Point Assignment Strategy:
            - Processes nodes level by level, from highest to lowest density
            - Assigns points in leaf nodes directly using node tokens
            - For non-leaf nodes:
                * Separates points into assigned and unassigned groups
                * Processes unassigned points based on density and proximity

        2. Assignment Rules:
            For leaf nodes:
                - Points get the node's token value directly
            For non-leaf nodes:
                - Unassigned points are handled in two ways:
                    a) If there are already assigned points:
                       * Assigns based on nearest assigned neighbor
                    b) If no assigned points but node has token:
                       * All points get the node's token value

        3. Ordering and Processing:
            - Orders unassigned points by density (highest first)
            - Ensures high-density points are assigned first
            - Maintains cluster connectivity through nearest neighbor assignments

        Notes:
        ------
        - Final cluster labels are remapped to start from 1
        - Points in the same contour get the same cluster label
        - Assignment preserves the density-based hierarchy
        - Method ensures all points get assigned to a cluster
        """
        # Initialize assignment array
        assignment = np.full(len(xy), None)

        # Find maximum level in the hierarchy
        max_level = -1
        for node in dic_node.keys():
            if dic_node[node]['level'] > max_level:
                max_level = dic_node[node]['level']

        # Process levels from highest to lowest
        for level in range(max_level, -1, -1):
            # Get nodes at current level
            list_node_level = []
            for node in dic_node.keys():
                if dic_node[node]['level'] == level:
                    list_node_level.append(node)

            # Process each node at current level
            for ind_node in list_node_level:
                # Handle leaf nodes
                if len(dic_node[ind_node]['childrens']) == 0:
                    if dic_node[ind_node]['token']:  # If node has tokens
                        for info_point in dic_point_node[ind_node]:
                            k = info_point[0]
                            assignment[k] = int(dic_node[ind_node]['token'][0])
                # Handle non-leaf nodes
                else:
                    # Initialize lists for assigned and unassigned points
                    already_assigned = []
                    not_assigned = []
                    already_assigned_value = []
                    already_assigned_point = []
                    not_assigned_point = []
                    not_assigned_den = []

                    # Split points into assigned and unassigned
                    for info_point in dic_point_node[ind_node]:
                        k = info_point[0]
                        if assignment[k] is not None:
                            already_assigned.append(k)
                            already_assigned_value.append(int(assignment[k]))
                            already_assigned_point.append(info_point[1])
                        else:
                            not_assigned.append(k)
                            not_assigned_point.append(info_point[1])
                            not_assigned_den.append(info_point[2])

                    # Process unassigned points if any exist
                    if len(not_assigned) != 0:
                        # Sort points by density in descending order
                        sorted_zip = sorted(zip(not_assigned_den, not_assigned, not_assigned_point), 
                                         key=lambda x: x[0], 
                                         reverse=True)
                        not_assigned_den, not_assigned, not_assigned_point = zip(*sorted_zip)

                        # Assign based on nearest assigned point or node token
                        if already_assigned:
                            already_assigned_point = np.array(already_assigned_point)
                            for ind in range(len(not_assigned)):
                                _, idx, _ = self.find_closest_point(not_assigned_point[ind], already_assigned_point)
                                assignment[not_assigned[ind]] = already_assigned_value[idx]
                        elif dic_node[ind_node]['token']:
                            for ind in range(len(not_assigned)):
                                assignment[not_assigned[ind]] = int(dic_node[ind_node]['token'][0])

        # Make cluster numbers start from 1
        if assignment is not None:
            unique_clusters = np.unique(assignment)
            mapping = {old: idx + 1 for idx, old in enumerate(unique_clusters)}
            assignment = np.array([mapping[x] for x in assignment])

        return assignment

    def run_clustering(self):
        """
        Executes the complete ClusterDC algorithm, implementing the full density-based 
        clustering workflow from contour creation to point assignment.

        Returns:
        --------
        tuple:
           - assignments: List of cluster assignments
               * For gap_order='all': List of multiple clustering solutions
               * For other modes: List containing single clustering solution
           - density_info: List containing [point_densities, grid_densities, xx, yy]
               Used for visualization and analysis

        Algorithm Workflow:
        ------------------
        1. Contour Analysis:
           - Creates density contours from KDE
           - Builds hierarchical structure of contours
           - Removes invalid leaves and chain nodes

        2. Peak Detection:
           - Identifies leaf nodes as potential peaks
           - Computes density values at peak centers
           - Creates graph representation of hierarchy

        3. Separability Analysis:
           - Calculates maximum flows between peaks
           - Determines well-separated points
           - Applies selected gap order criteria

        4. Cluster Assignment:
           For gap_order='all':
               - Generates multiple clustering solutions
               - Creates solutions from 1 to max clusters
               - Stores results in DataFrame with columns per solution

           For other modes:
               - Generates single clustering solution
               - Assigns points to clusters
               - Creates DataFrame with final assignments
        """

        # Create single progress bar for entire process
        with tqdm(total=100, desc="ClusterDC Clustering Progress", unit="%") as pbar:
            # Stage 1: Contour Analysis (25%)
            polygons, density_values, density_contours = self.create_contours()
            dic_node = self.make_node_dic(polygons, density_values, density_contours)
            dic_node = self.trim_unvalid_leaves(dic_node, polygons, self.min_point)
            dic_node = self.trim_chain_nodes(dic_node)
            pbar.update(25)

            # Stage 2: Peak Detection (25%)
            ind_peak = self.find_leaves(dic_node)
            peak_center, peak_center_density = self.compute_density_peak(polygons, ind_peak)
            G = self.create_graph(polygons, density_values, dic_node)
            pbar.update(25)

            # Stage 3: Flow Analysis (25%)
            max_flow, dens_list, dens_df = self.calculate_maximum_flow(G, peak_center_density, ind_peak)
            # Important change: pass print_table=False to delay printing
            ind_selection, separability_df = self.get_well_separated_points(max_flow, dens_list, dens_df, print_table=False)
            pbar.update(25)

            # Stage 4: Point Assignment (25%)
            assignments = []
            if self.gap_order == 'all':
                df = pd.DataFrame({'X': self.x_points, 'Y': self.y_points})
                for j in range(len(ind_selection)):
                    ind_selection_sub = ind_selection[:j + 1]
                    dic_node_sub = self.clean_dic_after_choice(ind_selection_sub, dic_node)
                    dic_point_node = self.get_point_polygon(polygons, dic_node_sub)
                    assignment_sub = self.assign_points(self.xy, dic_point_node, dic_node_sub)
                    assignments.append(assignment_sub)
                    df[f'{j+1} clusters'] = assignment_sub
            else:
                dic_node = self.clean_dic_after_choice(ind_selection, dic_node)
                dic_point_node = self.get_point_polygon(polygons, dic_node)
                assignment = self.assign_points(self.xy, dic_point_node, dic_node)
                assignments.append(assignment)
                df = pd.DataFrame({
                    'X': self.x_points,
                    'Y': self.y_points,
                    'Cluster': assignment
                })
            pbar.update(25)

        # Now that progress is 100%, print the separability table if available
        if separability_df is not None:
            print("\nSeparability and gaps:")
            display_output = separability_df[['separability', 'gap', 'gap_order']].fillna('--').round(6)
            print(display_output.to_string())

        print('\n============================')
        if self.gap_order == 'all':
            print('Max. number of clusters: ', len(np.unique(assignments[-1])))
        else:
            print('Number of clusters: ', len(np.unique(assignments[0])))
        print('============================\n')

        self.cluster_assignments = df
        density_info = [self.point_densities, self.grid_densities, self.xx, self.yy]

        return assignments, density_info
    
    def get_gap_position(self, max_flow, dens_list, dens_df):
        """
        Helper method to get the position based on gap order.

        Parameters:
            max_flow: List of maximum flow values
            dens_list: Array of density values
            dens_df: DataFrame containing peak information

        Returns:
            int: Position based on gap order
        """
        # Calculate separability for each point
        nb_values = len(max_flow)
        separability = np.array([1 - max_flow[i]/dens_list[i][0] for i in range(nb_values)])

        # Add separability to DataFrame and sort
        dens_df['separability'] = separability
        dens_df = dens_df.sort_values(by=['separability'], ascending=False)

        # Get positive separability points and add reference point
        well_separated = dens_df[dens_df['separability'] > 0].copy()
        well_separated.loc['*',:] = [0, 0]

        # Calculate gaps
        well_separated['gap'] = -well_separated.separability.diff()
        well_separated = well_separated.reset_index(drop=False)

        # Find gaps and their positions
        gaps = []
        for i in range(1, len(well_separated)):  # Start from 1 to skip reference point
            gap = well_separated.iloc[i]['gap']
            if not np.isnan(gap):
                gaps.append((abs(gap), i))

        # Sort gaps by size in descending order
        gaps.sort(reverse=True)

        if not gaps:
            print("\nNo gaps found in the data.")
            return 1

        if isinstance(self.gap_order, int):
            if self.gap_order > len(gaps):
                print(f"\nWarning: Requested gap order ({self.gap_order}) exceeds number of available gaps ({len(gaps)}).")
                print(f"Using the smallest available gap (gap order {len(gaps)}).")
                return gaps[-1][1]
            return gaps[self.gap_order - 1][1]
        elif self.gap_order == 'max_clusters':
            return len(well_separated) - 1  # Exclude reference point
        else:
            raise ValueError("Invalid gap_order parameter for get_gap_position method.")


    def get_cluster_assignments(self) -> pd.DataFrame:
        """
        Get cluster assignments for all points.

        Returns:
            pd.DataFrame: DataFrame containing point coordinates and cluster assignments
        
        Raises:
            RuntimeError: If clustering hasn't been performed yet
        """
        if self.cluster_assignments is None:
            raise RuntimeError("Clustering has not been performed yet. Run clustering first.")
        return self.cluster_assignments

    def plot_separability(self, save_path=None):
        """
        Creates a visualization of cluster separability analysis, showing the relationships
        between density peaks and the gaps that separate them.

        Parameters:
        -----------
        save_path : str, optional
           Path to save the plot as an image file. If None, plot is only displayed.

        Plot Components:
        ---------------
        1. Separability Curve:
           - Blue line with markers showing separability values
           - X-axis: Number of clusters
           - Y-axis: Separability values (0 to 1)
           - Higher values indicate better cluster separation

        2. Gap Visualization:
           - Red dashed vertical lines showing gaps
           - Gaps ordered by magnitude (Gap 1 = largest)
           - Labels showing gap number and magnitude
           - Positioned at midpoints between separability values

        3. Plot Elements:
           - Grid lines for better readability
           - Legend identifying curve and gaps
           - Axis labels and title
           - Automatic axis scaling with padding

        Visual Interpretation:
        ---------------------
        1. Separability Values:
           - Range from 0 to 1
           - 1 = Perfectly separated cluster
           - 0 = Complete connection to other clusters

        2. Gaps:
           - Represent natural breaks between clusters
           - Larger gaps suggest stronger cluster separation
           - Gap order indicates relative importance

        3. Pattern Analysis:
           - Sharp drops indicate clear cluster boundaries
           - Gradual changes suggest hierarchical structure
           - Flat regions indicate similar cluster strengths

        Notes:
        ------
        - Requires clustering to be run first (checks for _max_flow and _dens_list)
        - Automatically adjusts figure size and layout
        - Uses high-resolution output (300 dpi) when saving
        - Shows reference point (*) at zero for computational completeness

        Example Interpretation:
        ----------------------
        - Large first gap: Clear primary cluster separation
        - Multiple similar gaps: Several equally valid clustering options
        - Small gaps: Potential sub-cluster structure
        """
        if self._max_flow is None or self._dens_list is None:
            raise RuntimeError("No clustering results available. Run clustering first.")

        # Calculate separability values
        max_flow = self._max_flow
        dens_list = self._dens_list
        nb_values = len(max_flow)
        separability = np.array([1 - max_flow[i]/dens_list[i][0] for i in range(nb_values)])

        # Create DataFrame with separability values
        dens_df = self._dens_df.copy()
        dens_df['separability'] = separability
        dens_df = dens_df.sort_values(by=['separability'], ascending=False)

        # Get positive separability points and add reference point
        well_separated = dens_df[dens_df['separability'] > 0].copy()
        well_separated.loc['*',:] = [0, 0]

        # Calculate gaps
        well_separated['gap'] = -well_separated.separability.diff()
        well_separated = well_separated.reset_index(drop=False)

        # Sort gaps by magnitude (excluding first row which has NaN gap)
        gaps = well_separated['gap'].iloc[1:].to_numpy()
        gap_order = np.argsort(-gaps)  # Sort in descending order

        # Create the plot
        plt.figure(figsize=(12, 8))

        # Plot separability values
        x_values = np.arange(1, len(well_separated) + 1)
        plt.plot(x_values, well_separated['separability'], 'bo-', 
                 label='Separability', linewidth=2, markersize=8)

        # Add dummy plot for gap legend
        plt.plot([], [], 'r--', alpha=0.5, label='Gap Magnitude')

        # Plot gaps in order of magnitude
        for gap_num, gap_idx in enumerate(gap_order, 1):
            gap_idx = gap_idx + 1  # Adjust index since we excluded first row
            gap = well_separated.iloc[gap_idx]['gap']
            if not np.isnan(gap):
                # Calculate midpoint between points for vertical line
                mid_x = (x_values[gap_idx-1] + x_values[gap_idx]) / 2
                y_top = well_separated.iloc[gap_idx-1]['separability']
                y_bottom = well_separated.iloc[gap_idx]['separability']

                # Plot vertical line for gap
                plt.plot([mid_x, mid_x], [y_bottom, y_top], 'r--', alpha=0.5)

                # Add gap label
                plt.text(mid_x, y_bottom + (y_top - y_bottom)/2, 
                    f'Gap {gap_num}',
                    transform=plt.gca().transData,
                    rotation=45,
                    rotation_mode='anchor',
                    horizontalalignment='center',
                    verticalalignment='center',
                    fontsize=10,
                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, pad=1))

        # Customize the plot
        plt.xlabel('Number of Clusters', fontsize=12)
        plt.ylabel('Separability', fontsize=12)
        plt.title('Cluster Separability Analysis', fontsize=14, pad=20)

        # Set axis limits with padding
        plt.xlim(0.5, len(well_separated) + 0.5)
        y_min = min(0, well_separated['separability'].min()) - 0.1
        y_max = well_separated['separability'].max() + 0.1
        plt.ylim(y_min, y_max)

        # Add grid and legend
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.xticks(x_values)
        plt.legend(loc='upper right', fontsize=10)

        # Adjust layout
        plt.tight_layout()

        # Save if path provided
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()
   
        
    def plot_results(self, assignments, density_info, figsize=(20, 6), vmin=0):
        """
        Creates comprehensive visualization of clustering results, showing original data,
        density estimation, and cluster assignments.

        Parameters:
        -----------
        assignments : list
           List of cluster assignments:
           - For gap_order='all': Multiple clustering solutions
           - For other modes: Single clustering solution

        density_info : list
           List containing [point_densities, grid_densities, xx, yy]:
           - point_densities: Density values at each data point
           - grid_densities: Density values on regular grid
           - xx, yy: Grid coordinates for contour plotting

        figsize : tuple, optional
           Figure size in inches (width, height). Default is (20, 6)

        vmin : float, optional
           Minimum density value for visualization. Default is 0.01

        Plot Types:
        -----------
        1. For gap_order='all':
           Creates N+3 subplots, where N is number of clustering solutions:
           - Original data scatter plot
           - KDE density contour plot
           - Point density scatter plot
           - N clustering result plots (1 to N clusters)

        2. For other modes:
           Creates three subplots:
           - Original data (black points)
           - KDE density (contour plot with colorbar)
           - Clustering results (colored by cluster with legend)

        Visual Elements:
        ---------------
        1. Original Data:
           - Black scatter points
           - Consistent axis limits
           - Custom axis labels

        2. Density Visualization:
           - Contour plot with viridis colormap
           - Overlaid white points
           - Density colorbar
           - 20 contour levels

        3. Clustering Results:
           - Points colored by cluster
           - Automatic legend generation
           - Cluster count in title
           - Customized titles based on method

        Notes:
        ------
        - Maintains consistent axis limits across all plots
        - Automatically adjusts figure size for multiple solutions
        - Uses tab10 colormap for cluster visualization
        - Saves high-resolution output if save_path provided
        - Handles both hierarchical and single clustering modes
        - Creates legends for cluster identification
        """
        point_densities, grid_densities, xx, yy = density_info

        # Dynamically calculate vmin if not provided
        if vmin is None:
            # Find a small non-zero minimum, or use a very small value
            non_zero_densities = grid_densities[grid_densities > 0]
            vmin = non_zero_densities.min() if len(non_zero_densities) > 0 else 1e-10

        # Calculate axis limits from the grid
        x_min, x_max = xx[0, 0], xx[0, -1]
        y_min, y_max = yy[0, 0], yy[-1, 0]

        # Prepare contour levels
        max_density = np.max(grid_densities)

        # Create levels that are strictly increasing
        levels = np.linspace(vmin, max_density, 20)
        levels = np.unique(levels)  # Ensure unique, increasing values

        # Ensure at least two levels
        if len(levels) < 2:
            levels = [vmin, max_density]

        if self.gap_order == 'all':
            # Calculate required number of subplots
            n_plots = len(assignments) + 3  # +3 for original data, KDE density, and point densities
            n_cols = 3  # Fixed at 3 columns
            n_rows = (n_plots + n_cols - 1) // n_cols  # Ceiling division

            # Adjust figure size based on number of rows
            fig_height = 6 * n_rows
            plt.figure(figsize=(20, fig_height))

            # Plot original data
            plt.subplot(n_rows, n_cols, 1)
            plt.scatter(self.x_points, self.y_points, c='black', s=10)
            plt.title('Original Data')
            plt.xlabel(self.xlabel)
            plt.ylabel(self.ylabel)
            plt.xlim(x_min, x_max)
            plt.ylim(y_min, y_max)

            # Plot KDE density
            plt.subplot(n_rows, n_cols, 2)
            contour = plt.contourf(xx, yy, grid_densities, 
                                 levels=levels,
                                 cmap='viridis', 
                                 alpha=0.6)
            plt.colorbar(contour, label='KDE')
            plt.scatter(self.x_points, self.y_points, c='white', alpha=0.3, s=10)
            plt.title('KDE Density')
            plt.xlabel(self.xlabel)
            plt.ylabel(self.ylabel)
            plt.xlim(x_min, x_max)
            plt.ylim(y_min, y_max)

            # Plot point densities
            plt.subplot(n_rows, n_cols, 3)
            sc = plt.scatter(self.x_points, self.y_points, 
                            c=point_densities, 
                            cmap='viridis', 
                            s=10,
                            vmin=vmin,
                            vmax=np.max(point_densities))
            plt.colorbar(sc, label='Density')
            plt.title('Point Densities')
            plt.xlabel(self.xlabel)
            plt.ylabel(self.ylabel)
            plt.xlim(x_min, x_max)
            plt.ylim(y_min, y_max)

            # Plot all clustering results
            for i, assignment in enumerate(assignments):
                plt.subplot(n_rows, n_cols, i + 4)
                plt.scatter(self.x_points, self.y_points, c=assignment, cmap='tab10', s=10)
                plt.title(f'ClusterDC Clustering ({len(np.unique(assignment))} {"cluster" if len(np.unique(assignment)) == 1 else "clusters"})')
                plt.xlabel(self.xlabel)
                plt.ylabel(self.ylabel)
                plt.xlim(x_min, x_max)
                plt.ylim(y_min, y_max)

        else:  # For gap order, max_clusters, or user-specified number of clusters
            fig, ax = plt.subplots(1, 3, figsize=figsize)

            # Original Data plot
            ax[0].scatter(self.x_points, self.y_points, c='black', s=10)
            ax[0].set_title('Original Data')
            ax[0].set_xlabel(self.xlabel)
            ax[0].set_ylabel(self.ylabel)
            ax[0].set_xlim(x_min, x_max)
            ax[0].set_ylim(y_min, y_max)

            # KDE Density plot
            contour = ax[1].contourf(xx, yy, grid_densities, 
                                    levels=levels,
                                    cmap='viridis', 
                                    alpha=0.6)
            plt.colorbar(contour, ax=ax[1], label='KDE')
            ax[1].set_title('KDE Density')
            ax[1].set_xlabel(self.xlabel)
            ax[1].set_ylabel(self.ylabel)
            ax[1].set_xlim(x_min, x_max)
            ax[1].set_ylim(y_min, y_max)

            # Clustering results plot
            if assignments and len(assignments) > 0:  # Check if we have assignments
                assignment = assignments[-1]  # Use the last assignment

                # Create scatter plot for clusters
                scatter = ax[2].scatter(self.x_points, self.y_points, 
                                      c=assignment, cmap='tab10', s=10)

                # Get unique clusters and their colors
                unique_clusters = np.unique(assignment)
                colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))

                # Get unique clusters and their colors
                unique_clusters = np.unique(assignment)
                n_clusters = len(unique_clusters)
                colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))

                # Calculate number of legend columns needed (15 items per column)
                n_cols = (n_clusters - 1) // 15 + 1  # Integer division rounded up

                # Create legend handles
                legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                            markerfacecolor=colors[i], 
                                            label=f'Cluster {cluster}',
                                            markersize=10)
                                  for i, cluster in enumerate(unique_clusters)]

                # Add legend with dynamic columns
                if n_cols > 1:
                    # For multi-column legend, adjust bbox_to_anchor to prevent overlap
                    ax[2].legend(handles=legend_elements, 
                                loc='center left', 
                                bbox_to_anchor=(1, 0.5),
                                ncol=n_cols,
                                columnspacing=1.5,  # Add more space between columns
                                handletextpad=0.5)  # Reduce space between marker and text
                else:
                    # For single column legend, use original formatting
                    ax[2].legend(handles=legend_elements, 
                                loc='center left', 
                                bbox_to_anchor=(1, 0.5))

                # Set title based on clustering method
                if self.n_clusters is not None:
                    title = f'ClusterDC Clustering\n(User-specified {self.n_clusters} clusters)'
                elif self.gap_order == 'max_clusters':
                    title = f'ClusterDC Clustering\n(Maximum {len(np.unique(assignment))} clusters)'
                else:
                    title = f'ClusterDC Clustering\n({len(np.unique(assignment))} clusters)'

                ax[2].set_title(title)
                ax[2].set_xlabel(self.xlabel)
                ax[2].set_ylabel(self.ylabel)
                ax[2].set_xlim(x_min, x_max)
                ax[2].set_ylim(y_min, y_max)

        plt.tight_layout()

        # Save the figure if save_path is provided
        if self.save_path:
            plt.savefig(self.save_path, dpi=300, bbox_inches='tight')

        plt.show()
    

    def find_optimal_clusters(self, save_path=None, max_elbows=4):
        """
        Identifies potential elbow points in the cluster separability curve using Gaussian 
        Process Regression (GPR) to smooth the curve and analyze its curvature.

        This method:
        1. Extracts separability values from clustering results
        2. Fits a Gaussian Process Regression model to smooth the curve
        3. Identifies main regions with positive curvature where the curve is still decreasing
           (transitions from steep decline to more gradual decline)
        4. Visualizes the original data, GPR fit, curvature, and identified elbow points
        5. Returns a list of potential optimal cluster counts (limited to max_elbows)

        Parameters:
        -----------
        save_path : str, optional
            Path to save the plot. If None, plot is only displayed.
        max_elbows : int, optional
            Maximum number of elbow points to return (default: 4)

        Returns:
        --------
        list
            List of potential optimal cluster counts corresponding to main elbow points,
            sorted by curvature magnitude (most pronounced elbows first)

        Raises:
        -------
        RuntimeError
            If clustering has not been performed yet
        ImportError
            If required packages (scikit-learn, scipy) are not installed
        """
        # Check if clustering has been run
        if self._max_flow is None or self._dens_list is None:
            raise RuntimeError("No clustering results available. Run clustering first.")

        try:
            from sklearn.gaussian_process import GaussianProcessRegressor
            from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel
            from scipy.signal import find_peaks
            from scipy.ndimage import gaussian_filter1d
            import numpy as np
        except ImportError:
            raise ImportError("Required packages not installed. Please install scikit-learn and scipy.")

        # Calculate separability values
        max_flow = self._max_flow
        dens_list = self._dens_list
        nb_values = len(max_flow)
        separability = np.array([1 - max_flow[i]/dens_list[i][0] for i in range(nb_values)])

        # Create DataFrame with separability values
        dens_df = self._dens_df.copy()
        dens_df['separability'] = separability
        dens_df = dens_df.sort_values(by=['separability'], ascending=False)

        # Get positive separability points and add reference point
        well_separated = dens_df[dens_df['separability'] > 0].copy()
        well_separated.loc['*',:] = [0, 0]

        # Extract x and y values for GPR
        well_separated = well_separated.reset_index(drop=False)
        x_values = np.arange(1, len(well_separated)).reshape(-1, 1)  # Skip the reference point
        y_values = well_separated['separability'].iloc[1:].values  # Skip the reference point

        # If we have fewer than 5 clusters, just use the maximum number of clusters
        if len(x_values) < 5:
            optimal_clusters = len(x_values)
            print(f"Fewer than 5 potential clusters detected. Using maximum number of clusters: {optimal_clusters}")
            return [optimal_clusters]

        # Minimum data points required for GPR analysis
        if len(x_values) < 3:
            print("Not enough data points for GPR analysis. Need at least 3 points.")
            if len(x_values) > 0:
                optimal_clusters = len(x_values)
                print(f"Using the maximum available clusters: {optimal_clusters}")
                return [optimal_clusters]
            else:
                print("No valid clusters detected.")
                return []

        # Set up the Gaussian Process Regression with modified parameters to avoid overfitting
        # Use a larger length scale to capture the main trend rather than local variations
        # Add a WhiteKernel to handle noise explicitly
        length_scale = max(1.0, len(x_values) / 10)  # Scale based on data size

        kernel = C(1.0, (1e-2, 1e2)) * RBF(length_scale=length_scale, 
                                           length_scale_bounds=(length_scale/2, length_scale*2)) + \
                 WhiteKernel(noise_level=0.1, noise_level_bounds=(1e-3, 1.0))

        gpr = GaussianProcessRegressor(
            kernel=kernel, 
            n_restarts_optimizer=5,  # Fewer restarts to avoid finding local minima
            alpha=0.1,  # Add regularization
            normalize_y=True,  # Normalize target values
            random_state=42
        )

        # Fit the model to the data
        gpr.fit(x_values, y_values)

        # Create a fine-grained x-axis for prediction
        x_pred = np.linspace(x_values.min(), x_values.max(), 100).reshape(-1, 1)

        # Predict with GPR (mean and standard deviation)
        y_pred, y_std = gpr.predict(x_pred, return_std=True)

        # Calculate the first derivative (slope) of the curve
        dx = x_pred[1, 0] - x_pred[0, 0]
        first_derivative = np.gradient(y_pred, dx)

        # Calculate the second derivative (curvature) of the curve
        second_derivative = np.gradient(first_derivative, dx)

        # Apply stronger Gaussian smoothing to reduce noise and focus on main trends
        sigma = max(2.0, len(x_values) / 15)  # Scale smoothing based on data size
        smooth_second_derivative = gaussian_filter1d(second_derivative, sigma=sigma)

        # Only consider points with positive second derivative (convex regions)
        # and where first derivative is negative (still decreasing, but flattening)
        candidate_indices = np.where(
            (smooth_second_derivative > 0) & 
            (first_derivative < 0)
        )[0]

        # Initialize elbow_points
        elbow_points = []

        # Find significant positive curvature points
        if len(candidate_indices) > 0:
            # Set a minimum threshold for curvature to be considered significant
            # Use a percentage of the maximum positive curvature
            max_curvature = np.max(smooth_second_derivative[candidate_indices])
            curvature_threshold = max_curvature * 0.3  # Consider only top 30% of curvatures

            # Filter candidates by curvature threshold
            significant_candidates = [idx for idx in candidate_indices 
                                     if smooth_second_derivative[idx] >= curvature_threshold]

            # If too few points pass the threshold, take the top 1
            if len(significant_candidates) < 1 and len(candidate_indices) >= 1:
                # Sort candidates by curvature and take top 1
                significant_candidates = sorted(candidate_indices, 
                                              key=lambda idx: smooth_second_derivative[idx],
                                              reverse=True)[:1]

            # Sort candidate indices by curvature value
            sorted_candidates = sorted(significant_candidates, 
                                      key=lambda idx: smooth_second_derivative[idx],
                                      reverse=True)

            # Limit to max_elbows
            sorted_candidates = sorted_candidates[:max_elbows]

            # Get cluster counts for all candidates
            for idx in sorted_candidates:
                x_val = x_pred[idx, 0]
                cluster_count = int(np.round(x_val))
                # Ensure cluster count is within valid range
                cluster_count = max(1, min(cluster_count, len(x_values)))
                elbow_points.append((idx, cluster_count, smooth_second_derivative[idx]))

            # Filter out duplicates (when multiple points map to the same cluster count)
            unique_clusters = set()
            filtered_elbow_points = []
            for idx, count, curvature in elbow_points:
                if count not in unique_clusters:
                    unique_clusters.add(count)
                    filtered_elbow_points.append((idx, count, curvature))

            elbow_points = filtered_elbow_points

            # Further filter to remove adjacent cluster counts, keeping the one with higher curvature
            if len(elbow_points) > 1:
                # Sort by cluster count
                elbow_points.sort(key=lambda x: x[1])

                # Identify groups of adjacent clusters
                adjacent_groups = []
                current_group = [elbow_points[0]]

                for i in range(1, len(elbow_points)):
                    prev_count = elbow_points[i-1][1]
                    curr_count = elbow_points[i][1]

                    if curr_count - prev_count <= 1:  # Adjacent clusters
                        current_group.append(elbow_points[i])
                    else:
                        adjacent_groups.append(current_group)
                        current_group = [elbow_points[i]]

                # Add the last group
                if current_group:
                    adjacent_groups.append(current_group)

                # From each group, select the point with highest curvature
                filtered_again = []
                for group in adjacent_groups:
                    best_in_group = max(group, key=lambda x: x[2])
                    filtered_again.append(best_in_group)

                elbow_points = filtered_again
        else:
            print("No clear elbow points with positive curvature found.")

        # Create the visualization
        plt.figure(figsize=(12, 8))

        # Plot original data points
        plt.scatter(x_values, y_values, c='blue', s=50, label='Separability values')

        # Plot GPR prediction with uncertainty
        plt.plot(x_pred, y_pred, 'k-', label='GPR mean')
        plt.fill_between(x_pred.flatten(), 
                         y_pred - 1.96 * y_std, 
                         y_pred + 1.96 * y_std, 
                         alpha=0.2, 
                         color='k', 
                         label='GPR 95% confidence interval')

        # Create a secondary axis for showing the second derivative
        ax1 = plt.gca()
        ax2 = ax1.twinx()

        # Plot the second derivative (curvature) curve
        ax2.plot(x_pred, smooth_second_derivative, 'g-', alpha=0.7, label='Curvature (2nd derivative)')
        ax2.axhline(y=0, color='gray', linestyle=':', alpha=0.7)  # zero line for reference
        ax2.set_ylabel('Curvature', color='g')
        ax2.tick_params(axis='y', labelcolor='g')

        # Mark the positive curvature regions that meet the threshold
        if len(candidate_indices) > 0:
            # Only show candidate points if we have more than one elbow point
            if len(elbow_points) > 1:
                ax2.scatter(x_pred[candidate_indices], smooth_second_derivative[candidate_indices], 
                           c='lightgreen', alpha=0.3, s=20, marker='o')
            else:
                # If we only have one elbow point, don't clutter the plot with all candidates
                pass

        # Define colors and markers for identified elbow points
        markers = ['o', 's', 'd', '^']  # Different marker shapes
        colors = ['darkred', 'darkorange', 'darkgreen', 'darkblue']

        # Plot vertical lines for all elbow points
        for i, (idx, cluster_count, curvature) in enumerate(elbow_points):
            color_idx = i % len(colors)
            marker_idx = i % len(markers)
            x_val = x_pred[idx, 0]

            # Plot vertical line at elbow point
            plt.axvline(x=x_val, color=colors[color_idx], linestyle='--', alpha=0.4)

            # Mark the point on the curve
            plt.scatter([x_val], [y_pred[idx]], 
                        c=colors[color_idx], 
                        s=100, 
                        marker=markers[marker_idx],
                        label=f'Elbow point: {cluster_count} clusters', 
                        zorder=10-i)

            # Add small annotation with cluster count
            plt.annotate(f'{cluster_count}', 
                        (x_val, y_pred[idx]), 
                        xytext=(5, 5),
                        textcoords='offset points',
                        fontsize=9,
                        fontweight='bold')

        # Customize the plot
        ax1.set_xlabel('Number of Clusters', fontsize=12)
        ax1.set_ylabel('Separability', fontsize=12)
        plt.title('Elbow Point Analysis for Cluster Selection', fontsize=14, pad=20)

        # Set axis limits with padding
        plt.xlim(x_values.min() - 0.5, x_values.max() + 0.5)
        y_min = max(0, np.min(y_values) - 0.1)
        y_max = np.max(y_values) + 0.1
        plt.ylim(y_min, y_max)

        # Add grid and legend
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.xticks(np.arange(1, len(x_values) + 1))
        plt.legend(loc='upper right', fontsize=10)

        # Adjust layout
        plt.tight_layout()

        # Save if path provided
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

        # Print results
        if elbow_points:
            print(f"Main elbow points identified (clusters): {[count for _, count, _ in elbow_points]}")
        else:
            print("No clear elbow points identified.")

        # Return all identified elbow points as cluster counts
        return [count for _, count, _ in elbow_points]











import numpy as np
np.random.seed(42)
X1 = np.random.normal(loc=[0, 0], scale=[0.5, 0.5], size=(200, 2))
X2 = np.random.normal(loc=[5, 5], scale=[0.5, 0.5], size=(200, 2))
X3 = np.random.normal(loc=[-5, 5], scale=[0.5, 0.5], size=(200, 2))
X = np.vstack([X1, X2, X3])

import matplotlib.pyplot as plt

# Create the scatter plot
plt.figure(figsize=(10, 8))

# Plot each cluster with different colors
plt.scatter(X1[:, 0], X1[:, 1], c='blue', label='Cluster 1', alpha=0.6)
plt.scatter(X2[:, 0], X2[:, 1], c='red', label='Cluster 2', alpha=0.6)
plt.scatter(X3[:, 0], X3[:, 1], c='green', label='Cluster 3', alpha=0.6)

# Add labels and title
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Scatter Plot of Generated Clusters')

# Add legend
plt.legend()

# Add grid
plt.grid(True, alpha=0.3)

# Show plot
plt.show()





# Initialize and fit KDE with Global Bandwith Optimization
kde = KDE(
    data=X,
    kernel_types=['gaussian', 'epanechnikov', 'laplacian'],
    n_iter=100,
    save_path='Dataset1_global_bandwidth_KDE'
)

# Fit KDE with optimized global bandwidth
kde.fit(method='global_bandwidth')

# plot the progress in Bayesian optimization
kde.plot_optimization_progress(save_path='Dataset1_global_bandwidth_optimization_progress.png')  

# plot KDE outcomes 
kde.plot_results(vmin=0.01)  


# get the optimal kernel
kde.get_optimization_report()


# generate a report with all 100 Bayesian iterations with kernel parameters used and their objective values ranked 
# from best from worst fit
kde.get_iteration_results()


# Initialize and run ClusterDC using gap_order= max_cluster that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)

# plot separability plot
cluster_dc.plot_separability(save_path='Dataset1_global_bandwidth_separability_analysis.png')  # save_path is optional





# get a dataframe with clusters assignment
cluster_dc.get_cluster_assignments()


# Initialize and run ClusterDC gap_order=2 which generates clusters based on the second largest gap of separabilities
# in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=2, 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC gap_order='all' which generates all scanrios of clusters based on available gaps and 
# clusters
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='all', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# get a dataframe with clusters assignment
cluster_dc.get_cluster_assignments()


# Initialize and run ClusterDC and if you know the number of clusters that you want to generate then assigne None to 
# gap_order and n_clusters= the desired number of clusters, here we want to see 3 clusters
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None, 
    n_clusters = 3
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)








# Initialize and fit KDE with Scott's rule
kde = KDE(
    data=X,
)

# Fit KDE with global bandwidth's rule
kde.fit(method='scott')

# plot KDE outcomes 
kde.plot_results(vmin=0.01)  


# Initialize and run ClusterDC using max_cluster method that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters',
    save_path='Dataset1_Scott_ClusterDC.png'
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)

# plot separability plot
cluster_dc.plot_separability(save_path='Dataset1_Scott_separability_analysis.png')  # save_path is optional



# get a dataframe with clusters assignment
cluster_dc.get_cluster_assignments()


# Initialize and run ClusterDC gap_order=1 which generates clusters based on the largest gap of separabilities
# in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=1, 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC gap_order='all' which generates all scanrios of clusters based on available gaps and 
# clusters
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='all', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# get a dataframe with clusters assignment
cluster_dc.get_cluster_assignments()





# Initialize and fit KDE with local optimized bandwidth
kde = KDE(
    data=X,
    kernel_types=['gaussian', 'epanechnikov', 'laplacian'],
    n_iter=50
)

# Fit KDE with optimized local bandwidth
kde.fit(method='local_bandwidth')

# plot the progress in Bayesian optimization
kde.plot_optimization_progress(save_path='Dataset1_local_bandwidth_optimization_progress.png')  # Optional save_path

# plot KDE outcomes 
kde.plot_results() 


# get the optimal kernel
kde.get_optimization_report()


# generate a report with all 50 Bayesian iterations with kernel parameters used and their objective values ranked 
# from best from worst fit
kde.get_iteration_results()


# Initialize and run ClusterDC using max_cluster method that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# plot separability plot
cluster_dc.plot_separability(save_path='Dataset1_local_bandwidth_analysis.png')  # save_path is optional


# Find optimal number of clusters with the new method name
optimal_k = cluster_dc.find_optimal_clusters()


# get a dataframe with clusters assignment
cluster_dc.get_cluster_assignments()


# Initialize and run ClusterDC using gap_order=all method that finds the scanrios of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='all', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using gap_order=1 method that finds the clusters of the largest gap in separability 
# in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=1,
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using n_cluster=3 to find 3 clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None, 
    n_clusters=3
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)








data = Data()
df = data.read_file('training_data.csv')
df


data.get_summary()


data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', save_path='dataset2.png', show_kde=True)





# compare visually original data KDE agains sampled data KDE
df_density_samples = data.plot_density_samples(x='PaCMAP_X', y='PaCMAP_Y',n_samples=5000, return_samples=True)
df_density_samples


# create a new dataframe df_sample that selected randomly 5000 samples while ensuring low and high density areas are represented
# fairly in the new dataframe to expedite the analysis computational time.
df_sample = data.sample(n_samples=5000, method='density')
df_sample





#Initialize and fit KDE with Global Bandwith Optimization
kde = KDE(
    data=df_sample,
    kernel_types=['gaussian', 'epanechnikov', 'laplacian'],
    n_iter=50,
    save_path='Dataset2_global_bandwidth_KDE'
)

# Fit KDE with optimized global bandwidth
kde.fit(method='global_bandwidth')

# plot the progress in Bayesian optimization
kde.plot_optimization_progress(save_path='Dataset2_global_bandwidth_optimization_progress.png')  

#plot KDE outcomes 
kde.plot_results(vmin=0)  


# get the optimal kernel
kde.get_optimization_report()


# generate a report with all 100 Bayesian iterations with kernel parameters used and their objective values ranked 
# from best from worst fit
kde.get_iteration_results()


# Initialize and run ClusterDC using gap_order= max_cluster that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)

# plot separability plot
cluster_dc.plot_separability(save_path='Dataset2_global_bandwidth_separability_analysis.png')  # save_path is optional


# Assign labels to original data using KDTree nearest neighbor
df['cluster'] = data.assign_nearest_cluster(
    sampled_data=df_sample,
    cluster_labels=assignments[0],
    columns=['PaCMAP_X', 'PaCMAP_Y']
)

# You can also visualize it using the updated DataFrame
data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', labels='cluster')





# Initialize and fit KDE with Scott's rule
kde = KDE(
    data=df_sample,
)

# Fit KDE with global bandwidth's rule
kde.fit(method='scott')

# plot KDE outcomes 
kde.plot_results(vmin=0.01)  


# Initialize and run ClusterDC using max_cluster method that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters',
    save_path='Dataset2_Scott_ClusterDC.png'
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)

# plot separability plot
cluster_dc.plot_separability(save_path='Dataset2_Scott_separability_analysis.png')  # save_path is optional



# Assign labels to original data
df['cluster'] = data.assign_nearest_cluster(
    sampled_data=df_sample,
    cluster_labels=assignments[0],
    columns=['PaCMAP_X', 'PaCMAP_Y']
)

# You can also visualize it using the updated DataFrame
data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', labels='cluster')








# Initialize and fit KDE with local optimized bandwidth
kde = KDE(
    data=df_sample,
    kernel_types=['gaussian', 'epanechnikov', 'laplacian'],
    n_iter=50
)

# Fit KDE with optimized local bandwidth
kde.fit(method='local_bandwidth')

# plot the progress in Bayesian optimization
kde.plot_optimization_progress(save_path='Dataset2_local_bandwidth_optimization_progress.png')  # Optional save_path

# plot KDE outcomes 
kde.plot_results(vmin=0.0) 


# Initialize and run ClusterDC using max_cluster method that finds the maximum number of clusters in the data
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order='max_clusters', 
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# plot separability plot
cluster_dc.plot_separability(save_path='Dataset2_local_bandwidth_analysis.png')  # save_path is optional





# Find optimal number of clusters with the new method name
optimal_k = cluster_dc.find_optimal_clusters()


# Initialize and run ClusterDC using n_clusters=6 after following the recommendations of  find_optimal_clusters()
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None,
    n_clusters=6
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using n_clusters=2 
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None,
    n_clusters=2
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using n_clusters=3
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None,
    n_clusters=3
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using n_clusters=7 to find 7 clusters 
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=None,
    n_clusters=7
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using gap_order=1 method that finds the clusters with the largest gap in separability
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=1,
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using gap_order=2 method that finds the clusters with the largest gap in separability
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=2,
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using gap_order=3 method that finds the clusters with the largest gap in separability
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=3,
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Initialize and run ClusterDC using gap_order=8 method that finds the clusters with the largest gap in separability
cluster_dc = ClusterDC(
    kde_obj=kde,
    gap_order=8,
)

# Run clustering
assignments, density_info = cluster_dc.run_clustering()

#plot clusterdc outputs
cluster_dc.plot_results(assignments, density_info)


# Assign labels to original data using KDTree nearest 1 neighbor
df['cluster'] = data.assign_nearest_cluster(
    sampled_data=df_sample,
    cluster_labels=assignments[0],
    columns=['PaCMAP_X', 'PaCMAP_Y']
)

# You can also visualize it using the updated DataFrame
data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', labels='cluster')


# Assign labels to original data using KDTree nearest 3 neighbors
df['cluster'] = data.assign_nearest_cluster(
    sampled_data=df_sample,
    cluster_labels=assignments[0],
    columns=['PaCMAP_X', 'PaCMAP_Y'],
    k_neighbors=3
)

# Visualize the clustering results
data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', labels='cluster')


# Assign labels to original data using KDTree nearest 3 neighbors
df['cluster'] = data.assign_nearest_cluster(
    sampled_data=df_sample,
    cluster_labels=assignments[0],
    columns=['PaCMAP_X', 'PaCMAP_Y'],
    k_neighbors=20
)

# Visualize the clustering results
data.plot_scatter(x='PaCMAP_X', y='PaCMAP_Y', labels='cluster')











import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import seaborn as sns
from tqdm.notebook import tqdm  # For Jupyter notebook progress bar

def kde_benchmark_and_predict(data_file, sample_sizes, target_size=200000, method='scott', n_runs=3):
    """
    Comprehensive function that:
    1. Runs KDE benchmark on different sample sizes
    2. Fits a polynomial curve to the results
    3. Predicts processing time for larger datasets
    4. Creates visualizations of both benchmark results and predictions
    5. Generates detailed statistics on scaling behavior
    
    Parameters:
        data_file (str): Path to the data file
        sample_sizes (list): List of sample sizes to benchmark
        target_size (int): Size to predict processing time for
        method (str): KDE bandwidth method
        n_runs (int): Number of runs per sample size for averaging
        
    Returns:
        tuple: (results_df, polynomial_function, prediction_df)
    """
    # Part 1: Benchmarking
    # =====================
    
    # Function to measure KDE processing time
    def measure_kde_time(df, n_samples, method='scott', n_runs=3):
        """
        Measure the time needed to fit KDE with a specific sample size.
        """
        times = []
        
        for run in range(n_runs):
            # Sample the data using pandas DataFrame sample method
            df_sample = df.sample(n=n_samples, random_state=run)
            
            # Initialize KDE
            kde = KDE(data=df_sample)
            
            # Measure time to fit
            start_time = time.time()
            kde.fit(method=method)
            end_time = time.time()
            
            times.append(end_time - start_time)
            print(f"  Run {run+1}/{n_runs}: {times[-1]:.2f} seconds")
        
        avg_time = np.mean(times)
        print(f"  Average time for {n_samples} samples: {avg_time:.2f} seconds")
        return avg_time
    
    # Load data
    df = pd.read_csv(data_file)
    print(f"Successfully loaded data from {data_file}")
    print(f"Shape: {df.shape}")
    
    # Initialize results dataframe with explicit numeric dtypes
    results = pd.DataFrame({
        'sample_size': [],
        'time_seconds': []
    }, dtype=float)
    
    # Run benchmark for each sample size
    print(f"Running KDE benchmark with {method} method...")
    for size in sample_sizes:
        print(f"\nBenchmarking with {size:,} samples:")
        time_taken = measure_kde_time(df, size, method, n_runs)
        
        # Create a new row with explicit numeric types
        new_row = pd.DataFrame({
            'sample_size': [float(size)],
            'time_seconds': [float(time_taken)]
        })
        
        results = pd.concat([results, new_row], ignore_index=True)
    
    # Part 2: Fitting & Prediction
    # ============================
    
    # Convert data to numpy arrays
    x_data = results['sample_size'].to_numpy(dtype=float)
    y_data = results['time_seconds'].to_numpy(dtype=float)
    
    # Fit polynomial regression (degree 2)
    coefficients = np.polyfit(x_data, y_data, 2)
    polynomial = np.poly1d(coefficients)
    
    # Print the fitted polynomial
    print("\n==== KDE Timing Analysis ====")
    print(f"Fitted polynomial: {polynomial}")
    print(f"Formula: y = {coefficients[0]:.3e}x + {coefficients[1]:.3e}x + {coefficients[2]:.3f}")
    
    # Calculate R-squared
    y_mean = np.mean(y_data)
    ss_total = sum((y_data - y_mean) ** 2)
    ss_residual = sum((y_data - polynomial(x_data)) ** 2)
    r_squared = 1 - (ss_residual / ss_total)
    print(f"R-squared: {r_squared:.4f}")
    
    # Predict processing time for target size
    predicted_time = polynomial(target_size)
    print(f"\nPredicted processing time for {target_size:,} samples: {predicted_time:.2f} seconds")
    print(f"That's approximately {predicted_time/60:.1f} minutes or {predicted_time/3600:.2f} hours")
    
    # Part 3: Scaling Analysis
    # ========================
    
    print("\nScaling Factors Between Sample Sizes:")
    for i in range(1, len(sample_sizes)):
        size_ratio = float(sample_sizes[i]) / float(sample_sizes[i-1])
        time_ratio = float(results.iloc[i]['time_seconds']) / float(results.iloc[i-1]['time_seconds'])
        print(f"Size {sample_sizes[i-1]:,}  {sample_sizes[i]:,} ({size_ratio:.1f}x): Time increase {time_ratio:.2f}x")
    
    # Create predictions table for various sizes
    prediction_sizes = [20000, 50000, 100000, 150000, 200000, 250000, 500000]
    prediction_data = []
    
    print("\nPredictions for various sample sizes:")
    print("Sample Size    | Predicted Time (seconds) | Minutes   | Hours")
    print("-" * 65)
    
    for size in prediction_sizes:
        pred_time = polynomial(size)
        minutes = pred_time / 60
        hours = minutes / 60
        print(f"{size:12,} | {pred_time:24.2f} | {minutes:9.2f} | {hours:6.3f}")
        
        prediction_data.append({
            'sample_size': size,
            'seconds': pred_time,
            'minutes': minutes,
            'hours': hours
        })
    
    predictions_df = pd.DataFrame(prediction_data)
    
    # Part 4: Visualization
    # =====================
    
    # 1. Benchmark Results Plot
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=x_data, y=y_data, s=80, color='blue', alpha=0.7)
    
    # Add trend line
    x_trend = np.linspace(0, max(x_data), 100)
    plt.plot(x_trend, polynomial(x_trend), 'r--', linewidth=2)
    
    # Add annotations for the polynomial equation
    equation = f"y = {coefficients[0]:.2e}x + {coefficients[1]:.2e}x + {coefficients[2]:.2f}"
    plt.annotate(f"{equation}\nR = {r_squared:.3f}", 
                xy=(0.05, 0.95), xycoords='axes fraction',
                bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8))
    
    # Add labels and title
    plt.title(f'KDE Processing Time vs. Sample Size (Method: {method})', fontsize=14)
    plt.xlabel('Sample Size', fontsize=12)
    plt.ylabel('Processing Time (seconds)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.ticklabel_format(style='plain', axis='x')
    plt.ylim(bottom=0)
    plt.tight_layout()
    plt.savefig(f'kde_timing_benchmark_{method}.png', dpi=300)
    plt.show()
    
    # 2. Prediction Extrapolation Plot
    plt.figure(figsize=(12, 8))
    
    # Plot the original data points
    plt.scatter(x_data, y_data, s=100, color='blue', label='Benchmark data')
    
    # Plot the fitted curve for the range of measured data
    x_fit = np.linspace(0, max(x_data), 100)
    plt.plot(x_fit, polynomial(x_fit), 'r-', linewidth=2, label='Fitted curve')
    
    # Plot the extrapolation
    x_extrapolation = np.linspace(max(x_data), target_size, 100)
    plt.plot(x_extrapolation, polynomial(x_extrapolation), 'r--', linewidth=2, label='Extrapolation')
    
    # Mark the predicted point
    plt.scatter([target_size], [predicted_time], s=150, color='green', marker='*', 
                label=f'Prediction: {predicted_time:.1f} seconds')
    
    # Add labels and title
    plt.title('KDE Processing Time Extrapolation', fontsize=16)
    plt.xlabel('Sample Size', fontsize=14)
    plt.ylabel('Processing Time (seconds)', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)
    
    # Format axes
    plt.ticklabel_format(style='plain', axis='x')
    plt.ticklabel_format(style='plain', axis='y')
    
    # Add text annotation for the prediction
    plt.annotate(f"Predicted time for {target_size:,} samples:\n{predicted_time:.1f} seconds\n({predicted_time/60:.1f} minutes)",
                xy=(target_size, predicted_time), xytext=(target_size*0.7, predicted_time*0.6),
                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5, headwidth=8),
                bbox=dict(boxstyle="round,pad=0.5", fc="yellow", alpha=0.8),
                fontsize=12)
    
    plt.tight_layout()
    plt.savefig('kde_time_prediction.png', dpi=300)
    plt.show()
    
    # 3. Simple Plot of Benchmark Results
    plt.figure(figsize=(10, 6))
    plt.plot(results['sample_size'], results['time_seconds'], 'o-', linewidth=2, markersize=10)
    plt.title('KDE Processing Time vs. Sample Size', fontsize=14)
    plt.xlabel('Sample Size', fontsize=12)
    plt.ylabel('Processing Time (seconds)', fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('kde_timing_simple.png', dpi=300)
    plt.show()
    
    # 4. Prediction Bar Chart
    plt.figure(figsize=(12, 8))
    
    # Select a subset of sizes for better visualization
    sizes_for_plot = [20000, 50000, 100000, 200000, 500000]
    times_for_plot = [polynomial(size) for size in sizes_for_plot]
    
    # Create bar chart
    bars = plt.bar([str(size) for size in sizes_for_plot], times_for_plot, color='skyblue')
    
    # Add value labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        minutes = height / 60
        hours = minutes / 60
        
        if hours < 1:
            time_label = f"{minutes:.1f} min"
        else:
            time_label = f"{hours:.1f} hrs"
            
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02*max(times_for_plot),
                time_label, ha='center', va='bottom', fontsize=12)
    
    plt.title('Predicted KDE Processing Times for Various Sample Sizes', fontsize=16)
    plt.xlabel('Sample Size', fontsize=14)
    plt.ylabel('Processing Time (seconds)', fontsize=14)
    plt.grid(True, axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig('kde_predictions_barchart.png', dpi=300)
    plt.show()
    
    return results, polynomial, predictions_df



sample_sizes = [2000, 4000, 6000, 8000, 10000, 12000, 14000, 16000, 18000]
results, model, predictions = kde_benchmark_and_predict(
    'training_data.csv', 
    sample_sizes, 
    target_size=200000, 
    method='scott',
    n_runs=3
)



